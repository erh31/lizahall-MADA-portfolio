[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/lizahall/Desktop/School/Spring 2024/Applied Data Analysis/GitHub/lizahall-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   3   3     0        4          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  37.7 14.4  18  26  41  43   59 ▇▂▅▂▅\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\nb1&lt;- mydata %&gt;%\n  ggplot(mapping = aes(x = `Hair Color`, y = Height, fill = `Hair Color`)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"Blk\" = \"#1f78b4\", \"Bln\" = \"#33a02c\", \"Bro\" = \"#e31a1c\", \"Oth\" = \"#ff7f00\")) +\n  theme_minimal() +\n  labs(x = \"Hair Color\", y = \"Height\") +\n  ggtitle(\"Boxplot of Height by Hair Color\")\n  theme(plot.title = element_text(hjust = 0.5))  # Adjust title alignment\n\nList of 1\n $ plot.title:List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0.5\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\nb1\n\n\n\n  # Saving the figure of the boxplot\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=b1) \n\nSaving 7 x 5 in image\n\n\n\ns1&lt;- ggplot(mydata, aes(x = Weight, y = Age, color = \"red\")) +\n  geom_point() +\n  stat_smooth(method = \"glm\", formula = y ~ x) +\n  ggtitle(\"Scatterplot with Smoothed Line: Weight vs Age\") +\n  labs(x = \"Weight\", y = \"Age\")\ns1\n\n\n\n  # Saving the figure of the scatter plot\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=s1) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Placeholder file for the future data/results presentation exercise.\n\nThe Original Graph\n\nA Statistical Analysis of the Work of Bob Ross\n\nThis graph comes from an article titled “A Statistical Analysis of Bob Ross by FiveThirtyEight.\n\n\n\n\nRe-creation\nI utilized ChatGPT4 to help me recreate the original graph. Here are some of the prompts and responses:\nI asked the AI to help me recreate the original graph:\n\nInput:\n\nCan you help me to recreate this graph using R code? I’ve got the data and can provide more information about the dataset as needed. (Attached graph image)\n\n\n\nChatGPT:\n\nAbsolutely, I can help you recreate this graph using R. Could you please provide the dataset or the data points you have? Once I have the data, I can write and run the R code to generate a similar bar chart.\n\n\n\nInput:\n\n(Attached dataset ‘elements-by-episode.csv’)\n\nCode generated by the AI, first attempt:\n# Loading required libraries\nlibrary(here) \nlibrary(dplyr)  \nlibrary(ggplot2) \nlibrary(tidyverse)\nlibrary(gt)\n\n# Print the resolved file path\nprint(here(\"presentation-exercise\", \"elements-by-episode.csv\"))\n[1] “/Users/lizahall/Desktop/School/Spring 2024/Applied Data Analysis/GitHub/lizahall-MADA-portfolio/presentation-exercise/elements-by-episode.csv”\n# Load the data\ndata &lt;- read_csv(\"../presentation-exercise/elements-by-episode.csv\")\n\n# Calculate the percentage of episodes containing each element\npercentages &lt;- colSums(data[,-(1:2)]) / nrow(data) * 100\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(Element = names(percentages), Percentage = percentages) %&gt;%\n  arrange(desc(Percentage))\n\n# Plot the data using ggplot2\nggplot(plot_data, aes(x = reorder(Element, -Percentage), y = Percentage)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +  # Flip the coordinates to make a horizontal bar chart\n  theme_minimal() +\n  labs(title = \"The Paintings of Bob Ross\",\n       subtitle = \"Percentage containing each element\",\n       x = \"\",\n       y = \"Percentage\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) # Add a little space at the end of bars\n\nChatGPT seemed to have a lot of trouble with this one. Since the data had been manually grouped and not all of the variables were included it seemed to struggle to generate code that reproduced the results of the graph. However it did do a fairly good job capturing the general formatting after some more specific prompting.\n\n\nInput:\n\nCan you make the bars light blue, and also have the highest percentages on the top?\n\n\n\nChatGPT:\n\nCertainly! To make the bars light blue, you can specify the fill argument inside the geom_bar() function. Since the data is already sorted in descending order from the previous code, the highest percentages will be on the top. Here is the updated R code with these adjustments:\n\nCode generated by the AI, second attempt:\n# Load the data\ndata &lt;- read_csv(here(\"presentation-exercise\", \"elements-by-episode.csv\"))\n\n# Calculate the percentage of episodes containing each element\npercentages &lt;- colSums(data[,-(1:2)]) / nrow(data) * 100\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(Element = names(percentages), Percentage = percentages) %&gt;%\n  arrange(Percentage) # Sort in ascending order for correct plotting after coord_flip()\n\n# Plot the data using ggplot2 with light blue bars and correct order\nggplot(plot_data, aes(x = reorder(Element, Percentage), y = Percentage)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  coord_flip() +  # Flip the coordinates to make a horizontal bar chart\n  theme_minimal() +\n  labs(title = \"The Paintings of Bob Ross\",\n       subtitle = \"Percentage containing each element\",\n       x = \"\",\n       y = \"Percentage\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) # Add a little space at the end of bars\n\nWhile closer, it was still pretty far from the original graph. So the next edits were done manually. There was a lot of manual adjusting to be done.\n\n\n\n\nFinal Plot\nFinal attempt, after manual editing:\n# Load the filtered data\ndata &lt;- read_csv(here(\"presentation-exercise\", \"elements-by-episode.csv\"))\n\n# Updated list of categories with \"MOUNTAIN\" and optionally \"MOUNTAINS\"\ncategories &lt;- c(\"TREE\", \"TREES\", \"DECIDUOUS\", \"CONIFER\", \"CLOUDS\", \"CUMULUS\", \"CIRRUS\", \"LAKE\", \"RIVER\", \"SNOW\", \"MOUNTAIN\", \"MOUNTAINS\", \"GRASS\", \"BUSHES\", \"WATERFALL\", \"WINTER\", \"CABIN\", \"FRAMED\", \"PATH\", \"OVAL_FRAME\", \"OCEAN\", \"WAVES\", \"BEACH\", \"FENCE\", \"FOG\", \"HILLS\", \"BARN\", \"NIGHT\", \"FLOWERS\", \"PALM_TREES\", \"CLIFF\", \"BRIDGE\", \"STRUCTURE\", \"SNOWY_MOUNTAIN\", \"ROCKS\", \"SUN\") # Note: Adjust column names as needed based on actual dataset column names\n\n# Filter columns based on the updated list\nfiltered_data &lt;- select(data, all_of(categories))\n\n# Calculate the percentage of episodes each element appears in, excluding the EPISODE and TITLE columns\nelement_percentages &lt;- colSums(filtered_data) / nrow(filtered_data) * 100\n\n# Create a data frame for plotting\nelement_percentages_df &lt;- data.frame(Element = names(element_percentages), Percentage = element_percentages)\n\n# Sort the data frame in ascending order of percentage for correct ordering after coord_flip\nelement_percentages_df &lt;- element_percentages_df %&gt;%\n  arrange(Percentage)\n\n# Define the new names for the elements\nnew_names &lt;- c(\"Bridge\", \"Cliff\", \"Palm tree\", \"Nighttime\", \"Flowers\", \"Barn\", \"Hills\", \n               \"Fog\", \"Fence\", \"Beach\", \"Cirrus clouds\", \"Waves\", \"Ocean\", \"Oval frame\", \"Waterfall\", \"Sun\", \n               \"Path\", \"Frame\", \"Winter setting\", \"Cabin\", \"Snow\", \"Rocks\", \"Man-made structure\", \n               \"Cumulus clouds\", \"At least two mountains\", \"Snow-covered mountain\", \"Bushes\", \n               \"River or stream\", \"Grass\", \"Lake\", \"At least one mountain\", \"Clouds\", \"Coniferous tree\", \n               \"Deciduous tree\", \"At least two trees\", \"At least one tree\") \n\n# Replace the old names with the new names in the element_percentages_df dataframe\nelement_percentages_df$Element &lt;- new_names\n\n# Generate the bar chart with a more condensed aspect ratio and elements ordered correctly\ng &lt;- ggplot(element_percentages_df, aes(x = reorder(Element, Percentage), y = Percentage)) +\n  geom_bar(stat = 'identity', fill = 'skyblue', width = 0.5) +  # Reduce bar width\n  geom_text(aes(label = paste0(round(Percentage), \"%\")), hjust = -0.1, size = 3, color = \"black\") +  # Round percentages to whole numbers\n  coord_flip() +  # Flip the coordinates to get horizontal bars\n  labs(title = '', x = '', y = '') +  # Removed x axis label\n  ggtitle(\"The Paintings of Bob Ross\", subtitle = \"Percentage containing each element\") +  # Set title and subtitle\n  theme_minimal() +  # Use a minimal theme for a cleaner look\n  theme(axis.text.y = element_text(size = 5),  # Adjust text size for y axis\n        axis.title.y = element_blank(),  # Remove the y axis label\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank(),  # Remove minor grid lines\n        axis.text.x = element_blank(),  # Remove x-axis text\n        axis.ticks.x = element_blank(),  # Remove x-axis ticks\n        plot.title = element_text(hjust = 0, size = 14, face = \"bold\"),  # Adjust title size and position\n        plot.subtitle = element_text(hjust = 0, size = 10),  # Adjust subtitle size and position\n        plot.margin = margin(t = 1, r = 1, b = 1, l = 1, unit = \"cm\"))  # Adjust plot margin\n\n# Adjust plot aspect ratio and print the plot\nprint(g, vp = grid::viewport(width = 0.5, height = 1))\n\nWhile there was some variance in the results, overall the two were very similar in appearance and numbers.\n\n\n\nPublication style table\nUsing the code from the final graph, I prompted ChatGPT to help me generate a publication style table.\n\nInput:\n\n(Code for final table) Using the code above, can you generate me a publication style table using the ‘gt’ R package?\n\n\n\nChatGPT:\n\nTo create a nice-looking table using the gt package in R, we can start by preparing the data and then formatting it using gt’s styling options. Here’s how you can modify the provided code to create a visually pleasing table:\n\nCode generated by the AI, after more prompting and error correction:\n# Load the filtered data\ndata &lt;- read_csv(here(\"presentation-exercise\", \"elements-by-episode.csv\"))\n\n# Updated list of categories with \"MOUNTAIN\" and optionally \"MOUNTAINS\"\ncategories &lt;- c(\"TREE\", \"TREES\", \"DECIDUOUS\", \"CONIFER\", \"CLOUDS\", \"CUMULUS\", \"CIRRUS\", \"LAKE\", \"RIVER\", \"SNOW\", \"MOUNTAIN\", \"MOUNTAINS\", \"GRASS\", \"BUSHES\", \"WATERFALL\", \"WINTER\", \"CABIN\", \"FRAMED\", \"PATH\", \"OVAL_FRAME\", \"OCEAN\", \"WAVES\", \"BEACH\", \"FENCE\", \"FOG\", \"HILLS\", \"BARN\", \"NIGHT\", \"FLOWERS\", \"PALM_TREES\", \"CLIFF\", \"BRIDGE\", \"STRUCTURE\", \"SNOWY_MOUNTAIN\", \"ROCKS\", \"SUN\") # Note: Adjust column names as needed based on actual dataset column names\n\n# Filter columns based on the updated list\nfiltered_data &lt;- select(data, all_of(categories))\n\n# Calculate the percentage of episodes each element appears in, excluding the EPISODE and TITLE columns\nelement_percentages &lt;- colSums(filtered_data) / nrow(filtered_data) * 100\n\n# Create a data frame for the table\nelement_percentages_df &lt;- data.frame(Element = names(element_percentages), Percentage = element_percentages)\n\n# Sort the data frame in ascending order of percentage for correct ordering\nelement_percentages_df &lt;- element_percentages_df %&gt;%\n  arrange(Percentage)\n\n# Define the new names for the elements\nnew_names &lt;- c(\"Bridge\", \"Cliff\", \"Palm tree\", \"Nighttime\", \"Flowers\", \"Barn\", \"Hills\", \n               \"Fog\", \"Fence\", \"Beach\", \"Cirrus clouds\", \"Waves\", \"Ocean\", \"Oval frame\", \"Waterfall\", \"Sun\", \n               \"Path\", \"Frame\", \"Winter setting\", \"Cabin\", \"Snow\", \"Rocks\", \"Man-made structure\", \n               \"Cumulus clouds\", \"At least two mountains\", \"Snow-covered mountain\", \"Bushes\", \n               \"River or stream\", \"Grass\", \"Lake\", \"At least one mountain\", \"Clouds\", \"Coniferous tree\", \n               \"Deciduous tree\", \"At least two trees\", \"At least one tree\") \n\n# Replace the old names with the new names in the element_percentages_df dataframe\nelement_percentages_df$Element &lt;- new_names\n\n# Sort the data frame in descending order of percentage\nelement_percentages_df &lt;- element_percentages_df %&gt;%\n  arrange(desc(Percentage))\n\n\n# Updated script with corrected fmt_number() calls\ntable_gt &lt;- element_percentages_df %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"The Paintings of Bob Ross\",\n    subtitle = \"Percentage containing each element\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(\"Percentage\"),  # Updated to use c() instead of vars()\n    decimals = 1\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\", size = \"larger\")\n    ),\n    locations = cells_title(groups = \"title\")\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_text(style = \"italic\")\n    ),\n    locations = cells_title(groups = \"subtitle\")\n  )\n\nprint(table_gt)\n\n\n\n\n\n\n\nThe Paintings of Bob Ross\n\n\n\n\nPercentage containing each element\n\n\n\n\nElement\n\n\nPercentage\n\n\n\n\n\n\n\nAt least one tree\n\n\n89.6\n\n\n\n\nAt least two trees\n\n\n83.6\n\n\n\n\nDeciduous tree\n\n\n56.3\n\n\n\n\nConiferous tree\n\n\n52.6\n\n\n\n\nClouds\n\n\n44.4\n\n\n\n\nAt least one mountain\n\n\n39.7\n\n\n\n\nLake\n\n\n35.5\n\n\n\n\nGrass\n\n\n35.2\n\n\n\n\nRiver or stream\n\n\n31.3\n\n\n\n\nBushes\n\n\n29.8\n\n\n\n\nSnow-covered mountain\n\n\n27.0\n\n\n\n\nAt least two mountains\n\n\n24.6\n\n\n\n\nCumulus clouds\n\n\n21.3\n\n\n\n\nMan-made structure\n\n\n21.1\n\n\n\n\nRocks\n\n\n19.1\n\n\n\n\nSnow\n\n\n18.6\n\n\n\n\nWinter setting\n\n\n17.1\n\n\n\n\nCabin\n\n\n17.1\n\n\n\n\nFrame\n\n\n13.2\n\n\n\n\nPath\n\n\n12.2\n\n\n\n\nSun\n\n\n9.9\n\n\n\n\nWaterfall\n\n\n9.7\n\n\n\n\nOval frame\n\n\n9.4\n\n\n\n\nOcean\n\n\n8.9\n\n\n\n\nWaves\n\n\n8.4\n\n\n\n\nCirrus clouds\n\n\n6.9\n\n\n\n\nBeach\n\n\n6.7\n\n\n\n\nFence\n\n\n6.0\n\n\n\n\nFog\n\n\n5.7\n\n\n\n\nHills\n\n\n4.5\n\n\n\n\nBarn\n\n\n4.2\n\n\n\n\nFlowers\n\n\n3.0\n\n\n\n\nNighttime\n\n\n2.7\n\n\n\n\nPalm tree\n\n\n2.2\n\n\n\n\nCliff\n\n\n2.0\n\n\n\n\nBridge\n\n\n1.7"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#this-section-is-contributed-by-chaohua-li",
    "href": "cdcdata-exercise/cdcdata-exercise.html#this-section-is-contributed-by-chaohua-li",
    "title": "CDC Data Exercise",
    "section": "This section is contributed by Chaohua Li",
    "text": "This section is contributed by Chaohua Li"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#create-synthetic-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#create-synthetic-data",
    "title": "CDC Data Exercise",
    "section": "Create synthetic data",
    "text": "Create synthetic data\nWe create a new dataset by scrambling the data from the original dataset. That means the values in each variable are sampled from the old values without replacement. Since the year, month, and days are considered the id for each observation, these variables won’t be scrambled.\n\n#Create data set left that contains jurisdiction, year, month and days \nleft&lt;-cause_of_death_data_clean[,c(1:4)]\n#Create data set right that contains numbers of deaths for different causes\nright&lt;-cause_of_death_data_clean[,-c(1:4)]\n#set seed for reproducible results\nset.seed(456)\n#define a new data frame synth that will contain scrambled values\nsynth &lt;- right\n#use a loop to scramble values without replacement in the dataset right \nfor (col in colnames(right)) {\n  #sample values without replacement for each variable\n  synth[[col]] &lt;- sample(right[[col]], replace = FALSE)\n}\n#combine dataset left with the scrambled data right\nsynth2 &lt;- cbind(left, synth)"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#summarizes-and-explores-the-synthetic-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#summarizes-and-explores-the-synthetic-data",
    "title": "CDC Data Exercise",
    "section": "Summarizes and explores the synthetic data",
    "text": "Summarizes and explores the synthetic data\nCalculating percentages for the total count for each cause of death.\n\n# Calculate percentages total\ncause_counts_total &lt;- synth2 %&gt;%\n  select(-c(`All Cause`, Year, Month, `Number Of Days`)) %&gt;%\n  gather(key = \"Cause of Death\", value = \"count\", -`Jurisdiction of Occurrence`) %&gt;%\n  group_by(`Cause of Death`) %&gt;%\n  summarize(total_count = sum(count, na.rm = TRUE)) %&gt;%\n  mutate(percentage = total_count / sum(total_count) * 100) %&gt;%\n  arrange(desc(total_count))\n\nRecreating a pie chart for the overall total for each cause of death.\n\n# Create pie chart \npie_chart_total &lt;- ggplot(cause_counts_total, aes(x = \"\", y = total_count, fill = `Cause of Death`)) +\n  geom_bar(stat = \"identity\") +\n  coord_polar(\"y\", start = 0) +\n  labs(title = \"Synthetic Data: Distribution of \\nCauses of Death\",\n       fill = \"Cause of Death\",\n       x = NULL, y = NULL,) +\n  theme_void() +\n  theme(legend.position = \"right\",\n        legend.text = element_text(size = 8),  \n        legend.title = element_text(size = 10),  \n        legend.key.size = unit(0.5, \"lines\"), \n        plot.title = element_text(size = 16),  \n        plot.margin = margin(2, 6, 2, 2, \"cm\"),\n        legend.box.margin = margin(0, -10, 0, 0)) +  \n  guides(fill = guide_legend(\n    keywidth = unit(0.5, \"lines\"),  \n    label.position = \"right\",       \n    label.hjust = 0                 \n  )) +\n  scale_fill_discrete(labels = paste0(cause_counts_total$`Cause of Death`, \" (\", round(cause_counts_total$percentage), \"%)\"))\n\n# Adjustments\npie_chart_total &lt;- pie_chart_total + theme(\n  plot.margin = margin(2, 2, 2, 2, \"cm\"),\n  plot.title = element_text(size = 16, hjust = 0.5, margin = margin(0, 0, 10, 0)),\n  plot.caption = element_text(size = 10, hjust = 0.5, margin = margin(10, 0, 0, 0))\n)\n\n# Show pie chart\nprint(pie_chart_total)\n\n\n\n\nBecause each cause of death in the synthetic dataset contains exactly the same group of values, so the totals for each cause are the same with the original dataset.\n\nGrouping by month and cause of death, calculating total deaths per month, and calculating the percentage of total deaths each month. This is for graphing purposes.\n\n# Group by month and cause of death\ncause_counts_month &lt;- synth2 %&gt;%\n  select(-c(`All Cause`, Year, `Number Of Days`)) %&gt;%\n  gather(key = \"Cause of Death\", value = \"count\", -`Jurisdiction of Occurrence`, -Month) %&gt;%\n  group_by(Month, `Cause of Death`) %&gt;%\n  summarize(total_count = sum(count, na.rm = TRUE)) %&gt;%\n  mutate(Month = factor(month.name[Month], levels = month.name)) %&gt;%\n  arrange(Month, desc(total_count))\n\n# Calculate total deaths for each month\ntotal_deaths_month &lt;- cause_counts_month %&gt;%\n  group_by(Month) %&gt;%\n  summarise(total_deaths = sum(total_count))\n\n# Calculate percentage of total deaths for each month\ntotal_deaths_month &lt;- total_deaths_month %&gt;%\n  mutate(percentage = total_deaths / sum(total_deaths) * 100)\n\nPlot stacked bar plot for causes of death per month.\n\n# Create a stacked bar plot for causes of death by month\nggplot(cause_counts_month, aes(x = Month, y = total_count/1e6, fill = `Cause of Death`)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(labels = function(x) paste0(format(x, big.mark = \",\", scientific = FALSE), \" million\"), \n                     breaks = pretty_breaks()) + \n  labs(title = \"Synthetic Data: Total Causes of \\nDeath by Month\",\n       x = \"Month\",\n       y = \"Total Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\nThis bar plot looks different from the one using original data, because the random sampling broke the association between month and deaths due to different causes.\nPlot bar graph for total number of deaths per month.\n\n# Plot the bar graph for total number of deaths per month.\nggplot(total_deaths_month, aes(x = Month, y = total_deaths/1e6, fill = Month)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(labels = function(x) paste0(format(x, big.mark = \",\", scientific = FALSE), \" million\"), \n                     breaks = pretty_breaks()) +\n  labs(title = \"Synthetic Data:Total Deaths by Month\",\n       x = \"Month\",\n       y = \"Total Deaths\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPrint table for total deaths per month.\n\n# Print table of total deaths per month.\nkable(total_deaths_month, \n      col.names = c(\"Month\", \"Total Deaths\", \"Percentage\"),\n      format = \"html\",\n      digits = 2,\n      caption = \"Synthetic Data:Total Deaths and Percentage by Month\") %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  scroll_box(height = \"200px\")\n\n\n\nSynthetic Data:Total Deaths and Percentage by Month\n\n\nMonth\nTotal Deaths\nPercentage\n\n\n\n\nJanuary\n1431480\n7.61\n\n\nFebruary\n1595078\n8.48\n\n\nMarch\n1465555\n7.79\n\n\nApril\n1480724\n7.87\n\n\nMay\n1504505\n7.99\n\n\nJune\n1646561\n8.75\n\n\nJuly\n1750683\n9.30\n\n\nAugust\n1623654\n8.63\n\n\nSeptember\n1619599\n8.61\n\n\nOctober\n1564062\n8.31\n\n\nNovember\n1572547\n8.36\n\n\nDecember\n1564330\n8.31\n\n\n\n\n\n\n\nDifferences in the distribution of deaths by month between original and synthetic data are also reflected in this table.\n\nGrouping by year and cause of death, calculating total deaths per year, and calculating the percentage of total deaths each year. This is for graphing purposes.\n\n# Group by year and cause of death\ncause_counts_year &lt;- synth2 %&gt;%\n  select(-c(`All Cause`, Month, `Number Of Days`)) %&gt;%\n  gather(key = \"Cause of Death\", value = \"count\", -`Jurisdiction of Occurrence`, -Year) %&gt;%\n  group_by(Year, `Cause of Death`) %&gt;%\n  summarize(total_count = sum(count, na.rm = TRUE)) %&gt;%\n  arrange(Year, desc(total_count))\n\n# Calculate total deaths for each year\ntotal_deaths_year &lt;- cause_counts_year %&gt;%\n  group_by(Year) %&gt;%\n  summarise(total_deaths = sum(total_count))\n\n# Calculate percentage of total deaths for each year\ntotal_deaths_year &lt;- total_deaths_year %&gt;%\n  mutate(percentage = total_deaths / sum(total_deaths) * 100)\n\nPlot bar graph for total number of death per year.\n\n# Plot bar graph for total number of deaths per year.\nggplot(total_deaths_year, aes(x = Year, y = total_deaths/1e6, fill = as.factor(Year))) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(labels = function(x) paste0(format(x, big.mark = \",\", scientific = FALSE), \" million\"), \n                     breaks = pretty_breaks()) +\n  labs(title = \"Synthetic Data:Total Deaths by Year\",\n       x = \"Year\",\n       y = \"Total Deaths\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\nThe distribution of deaths by year using synthetic data is very similar to that in the original data. But the table below does show the numbers are slightly different from the original results.\n\nPrint table for total deaths per year.\n\n# Print table for total deaths per year.\nkable(total_deaths_year, \n      col.names = c(\"Year\", \"Total Deaths\", \"Percentage\"),\n      format = \"html\",\n      digits = 2,\n      caption = \"Synthetic Data:Total Deaths and Percentage by Year\") %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  scroll_box(height = \"200px\")\n\n\n\nSynthetic Data:Total Deaths and Percentage by Year\n\n\nYear\nTotal Deaths\nPercentage\n\n\n\n\n2020\n6193867\n32.91\n\n\n2021\n6531616\n34.71\n\n\n2022\n6093295\n32.38\n\n\n\n\n\n\n\n\nGrouping by month, year, and cause of death, calculating total deaths per year per month, and calculating the percentage of total deaths each year and month. This is for graphing purposes.\n\n# Group by year, month, and cause of death\ncause_counts_year_month &lt;- synth2 %&gt;%\n  select(-c(`All Cause`, `Number Of Days`)) %&gt;%\n  gather(key = \"Cause of Death\", value = \"count\", -`Jurisdiction of Occurrence`, -Year, -Month) %&gt;%\n  group_by(Year, Month, `Cause of Death`) %&gt;%\n  summarize(total_count = sum(count, na.rm = TRUE)) %&gt;%\n  arrange(Year, Month, desc(total_count))\n\n# Calculate total deaths for each year and month\ntotal_deaths_year_month &lt;- cause_counts_year_month %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(total_deaths = sum(total_count))\n\n# Calculate percentage of total deaths for each year and month\ntotal_deaths_year_month &lt;- total_deaths_year_month %&gt;%\n  mutate(percentage = total_deaths / sum(total_deaths) * 100)\n\nPlot stacked bar plot for causes of death per month per year.\n\n# Create a stacked bar plot for causes of death by month and year\nggplot(cause_counts_year_month, aes(x = Month, y = total_count/1e6, fill = `Cause of Death`)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~Year) +  # facet by year\n  scale_y_continuous(labels = function(x) paste0(format(x, big.mark = \",\", scientific = FALSE), \" million\"), \n                     breaks = pretty_breaks()) + # format y-axis labels\n  labs(title = \"Synthetic Data:Total Causes of Death \\nby Month & Year\",\n       x = \"Month\",\n       y = \"Total Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\nThe distribution of deaths by year and month using synthetic data looks different from the original results. This is due to the random sampling process which broke the original pattern.\n\nPrint table for total deaths per month per year.\n\n# Print table for total deaths per month per year.\nkable(total_deaths_year_month, \n      col.names = c(\"Year\", \"Month\", \"Total Deaths\", \"Percentage\"),\n      format = \"html\",\n      digits = 2,\n      caption = \"Synthetic Data:Total Deaths and Percentage \\nby Year and Month\") %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  scroll_box(height = \"600px\")\n\n\n\nSynthetic Data:Total Deaths and Percentage by Year and Month\n\n\nYear\nMonth\nTotal Deaths\nPercentage\n\n\n\n\n2020\n1\n472058\n7.62\n\n\n2020\n2\n497446\n8.03\n\n\n2020\n3\n524118\n8.46\n\n\n2020\n4\n500293\n8.08\n\n\n2020\n5\n518146\n8.37\n\n\n2020\n6\n561612\n9.07\n\n\n2020\n7\n492192\n7.95\n\n\n2020\n8\n477343\n7.71\n\n\n2020\n9\n589829\n9.52\n\n\n2020\n10\n526264\n8.50\n\n\n2020\n11\n528635\n8.53\n\n\n2020\n12\n505931\n8.17\n\n\n2021\n1\n488815\n7.48\n\n\n2021\n2\n606530\n9.29\n\n\n2021\n3\n461131\n7.06\n\n\n2021\n4\n478082\n7.32\n\n\n2021\n5\n537713\n8.23\n\n\n2021\n6\n561448\n8.60\n\n\n2021\n7\n703027\n10.76\n\n\n2021\n8\n524714\n8.03\n\n\n2021\n9\n546985\n8.37\n\n\n2021\n10\n517421\n7.92\n\n\n2021\n11\n532602\n8.15\n\n\n2021\n12\n573148\n8.77\n\n\n2022\n1\n470607\n7.72\n\n\n2022\n2\n491102\n8.06\n\n\n2022\n3\n480306\n7.88\n\n\n2022\n4\n502349\n8.24\n\n\n2022\n5\n448646\n7.36\n\n\n2022\n6\n523501\n8.59\n\n\n2022\n7\n555464\n9.12\n\n\n2022\n8\n621597\n10.20\n\n\n2022\n9\n482785\n7.92\n\n\n2022\n10\n520377\n8.54\n\n\n2022\n11\n511310\n8.39\n\n\n2022\n12\n485251\n7.96\n\n\n\n\n\n\n\n=======\n\nAbout the Data\nThis dataset is the ‘Monthly Provisional Counts of Deaths by Select Causes, 2020-2023’ though for the purpose of this exercise I am only using data from 2020-2022.\nThe dataset can be found here:\nhttps://data.cdc.gov/NCHS/Monthly-Provisional-Counts-of-Deaths-by-Select-Cau/9dzk-mvmi/about_data\nAfter cleaning the dataset contains the following list of variables:\n\nJurisdiction of Occurrence\nYear\nMonth\nNumber Of Days\nAll Cause\nNatural Cause\nSepticemia\nMalignant Neoplasms\nDiabetes Mellitus\nAlzheimer Disease\nInfluenza and Pneumonia\nChronic Lower Respiratory Diseases\nOther Diseases of Respiratory System\nNephritis/Nephrotic Syndrome and Nephrosis\nAbnormal Findings (No Classifiable Diagnosis)\nDiseases of Heart\nCerebrovascular Diseases\nAccidents/Unintentional Injuries\nMotor Vehicle Accidents\nIntentional Self Harm/Suicide\nAssault/Homicide\nDrug Overdose\nCOVID 19/Multiple Cause of Death\nCOVID 19/Underlying Cause of Death\n\n\n\nCleaning the Dataset\nLoad required package and load dataset.\n# load libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(here)\n\n# Specify the file path relative to the working directory\nfile_path &lt;- \"cdcdata-exercise/causeofdeathdata.csv\"\n\n# Load the CSV file into a data frame\ncause_of_death_data_clean &lt;- read.csv(here(\"cdcdata-exercise\", \"causeofdeathdata.csv\"), stringsAsFactors = FALSE)\nCreating a new variable, to prepare for the removal of redundant variables in the next step.\n\n# Creating Number.Of.Days variable so that Start.Date and End.Date can be removed\n# Month and Year are already variables, so Start.Date and End.Date become somewhat redundant\ncause_of_death_data_clean$Number.Of.Days &lt;- as.numeric(\n  as.Date(cause_of_death_data_clean$End.Date, format = \"%m/%d/%Y\") - \n    as.Date(cause_of_death_data_clean$Start.Date, format = \"%m/%d/%Y\")\n)\ncause_of_death_data_clean &lt;- cause_of_death_data_clean %&gt;%\n  dplyr::select(Jurisdiction.of.Occurrence, Year, Month, Number.Of.Days, everything())\n\nRemoving variables which contain junk text, and also getting rid of rows which contain no data. I also chose to filter out all data from 2023, since it was incomplete.\n\n# Removing variables which display only 'Data not shown (6 month lag)'\n# Removing Start.Date, End.Date, and Data.As.Of variables\ncause_of_death_data_clean &lt;- subset(cause_of_death_data_clean, select = -c(flag_accid, flag_mva, flag_suic, flag_homic, flag_drugod,Start.Date,End.Date,Data.As.Of))\n\n# Removing rows with any NA values\ncause_of_death_data_clean &lt;- cause_of_death_data_clean[complete.cases(cause_of_death_data_clean), ]\n\n# Filtering out data from the year 2023 because it is incomplete\ncause_of_death_data_clean &lt;- cause_of_death_data_clean %&gt;%\n  filter(Year != 2023)\n\nBecause of how variable names were formatted within the dataset, I added some code to make them more readable. I also altered the name of one variable which was very long and not practical for display purposes.\n\n# Cleaning up variable names\n\nclean_variable_names &lt;- function(name) {\n  name &lt;- gsub(\"\\\\.+\", \" \", gsub(\"\\\\.\\\\.\", \"/\", name))\n  name &lt;- gsub(\"Symptoms/Signs and Abnormal Clinical and Laboratory Findings/Not Elsewhere Classified\", \"Abnormal Findings (No Classifiable Diagnosis)\", name)\n  return(name)\n}\n\ncause_of_death_data_clean &lt;- cause_of_death_data_clean %&gt;%\n  rename_with(clean_variable_names, everything())\n\n\n\nVisualizing the Data\nCalculating percentages for the total count for each cause of death.\n\n# Calculate percentages total\ncause_counts_total &lt;- cause_of_death_data_clean %&gt;%\n  select(-c(`All Cause`, Year, Month, `Number Of Days`)) %&gt;%\n  gather(key = \"Cause of Death\", value = \"count\", -`Jurisdiction of Occurrence`) %&gt;%\n  group_by(`Cause of Death`) %&gt;%\n  summarize(total_count = sum(count, na.rm = TRUE)) %&gt;%\n  mutate(percentage = total_count / sum(total_count) * 100) %&gt;%\n  arrange(desc(total_count))\n\nCreating a pie chart for the overall total for each cause of death.\n\n# Create pie chart \npie_chart_total &lt;- ggplot(cause_counts_total, aes(x = \"\", y = total_count, fill = `Cause of Death`)) +\n  geom_bar(stat = \"identity\") +\n  coord_polar(\"y\", start = 0) +\n  labs(title = \"Distribution of Causes of Death\",\n       fill = \"Cause of Death\",\n       x = NULL, y = NULL,\n       caption = \"Data source: CDC\") +\n  theme_void() +\n  theme(legend.position = \"right\",\n        legend.text = element_text(size = 8),  \n        legend.title = element_text(size = 10),  \n        legend.key.size = unit(0.5, \"lines\"), \n        plot.title = element_text(size = 16),  \n        plot.margin = margin(2, 6, 2, 2, \"cm\"),\n        legend.box.margin = margin(0, -10, 0, 0)) +  \n  guides(fill = guide_legend(\n    keywidth = unit(0.5, \"lines\"),  \n    label.position = \"right\",       \n    label.hjust = 0                 \n  )) +\n  scale_fill_discrete(labels = paste0(cause_counts_total$`Cause of Death`, \" (\", round(cause_counts_total$percentage), \"%)\"))\n\n# Adjustments\npie_chart_total &lt;- pie_chart_total + theme(\n  plot.margin = margin(2, 2, 2, 2, \"cm\"),\n  plot.title = element_text(size = 16, hjust = 0.5, margin = margin(0, 0, 10, 0)),\n  plot.caption = element_text(size = 10, hjust = 0.5, margin = margin(10, 0, 0, 0))\n)\n\n# Show pie chart\nprint(pie_chart_total)\n\n\n\n\n\nGrouping by month and cause of death, calculating total deaths per month, and calculating the percentage of total deaths each month. This is for graphing purposes.\n\n# Group by month and cause of death\ncause_counts_month &lt;- cause_of_death_data_clean %&gt;%\n  select(-c(`All Cause`, Year, `Number Of Days`)) %&gt;%\n  gather(key = \"Cause of Death\", value = \"count\", -`Jurisdiction of Occurrence`, -Month) %&gt;%\n  group_by(Month, `Cause of Death`) %&gt;%\n  summarize(total_count = sum(count, na.rm = TRUE)) %&gt;%\n  mutate(Month = factor(month.name[Month], levels = month.name)) %&gt;%\n  arrange(Month, desc(total_count))\n\n# Calculate total deaths for each month\ntotal_deaths_month &lt;- cause_counts_month %&gt;%\n  group_by(Month) %&gt;%\n  summarise(total_deaths = sum(total_count))\n\n# Calculate percentage of total deaths for each month\ntotal_deaths_month &lt;- total_deaths_month %&gt;%\n  mutate(percentage = total_deaths / sum(total_deaths) * 100)\n\nPlot stacked bar plot for causes of death per month.\n\n# Create a stacked bar plot for causes of death by month\nggplot(cause_counts_month, aes(x = Month, y = total_count/1e6, fill = `Cause of Death`)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(labels = function(x) paste0(format(x, big.mark = \",\", scientific = FALSE), \" million\"), \n                     breaks = pretty_breaks()) + \n  labs(title = \"Total Causes of Death by Month\",\n       x = \"Month\",\n       y = \"Total Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nPlot bar graph for total number of deaths per month.\n\n# Plot the bar graph for total number of deaths per month.\nggplot(total_deaths_month, aes(x = Month, y = total_deaths/1e6, fill = Month)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(labels = function(x) paste0(format(x, big.mark = \",\", scientific = FALSE), \" million\"), \n                     breaks = pretty_breaks()) +\n  labs(title = \"Total Deaths by Month\",\n       x = \"Month\",\n       y = \"Total Deaths\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPrint table for total deaths per month.\n\n# Print table of total deaths per month.\nkable(total_deaths_month, \n      col.names = c(\"Month\", \"Total Deaths\", \"Percentage\"),\n      format = \"html\",\n      digits = 2,\n      caption = \"Total Deaths and Percentage by Month\") %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  scroll_box(height = \"200px\")\n\n\n\nTotal Deaths and Percentage by Month\n\n\nMonth\nTotal Deaths\nPercentage\n\n\n\n\nJanuary\n1973713\n10.49\n\n\nFebruary\n1531031\n8.14\n\n\nMarch\n1452402\n7.72\n\n\nApril\n1537652\n8.17\n\n\nMay\n1442009\n7.66\n\n\nJune\n1324227\n7.04\n\n\nJuly\n1440256\n7.65\n\n\nAugust\n1560359\n8.29\n\n\nSeptember\n1527357\n8.12\n\n\nOctober\n1537261\n8.17\n\n\nNovember\n1595771\n8.48\n\n\nDecember\n1896740\n10.08\n\n\n\n\n\n\n\n\nGrouping by year and cause of death, calculating total deaths per year, and calculating the percentage of total deaths each year. This is for graphing purposes.\n\n# Group by year and cause of death\ncause_counts_year &lt;- cause_of_death_data_clean %&gt;%\n  select(-c(`All Cause`, Month, `Number Of Days`)) %&gt;%\n  gather(key = \"Cause of Death\", value = \"count\", -`Jurisdiction of Occurrence`, -Year) %&gt;%\n  group_by(Year, `Cause of Death`) %&gt;%\n  summarize(total_count = sum(count, na.rm = TRUE)) %&gt;%\n  arrange(Year, desc(total_count))\n\n# Calculate total deaths for each year\ntotal_deaths_year &lt;- cause_counts_year %&gt;%\n  group_by(Year) %&gt;%\n  summarise(total_deaths = sum(total_count))\n\n# Calculate percentage of total deaths for each year\ntotal_deaths_year &lt;- total_deaths_year %&gt;%\n  mutate(percentage = total_deaths / sum(total_deaths) * 100)\n\nPlot bar graph for total number of death per year.\n\n# Plot bar graph for total number of deaths per year.\nggplot(total_deaths_year, aes(x = Year, y = total_deaths/1e6, fill = as.factor(Year))) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(labels = function(x) paste0(format(x, big.mark = \",\", scientific = FALSE), \" million\"), \n                     breaks = pretty_breaks()) +\n  labs(title = \"Total Deaths by Year\",\n       x = \"Year\",\n       y = \"Total Deaths\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPrint table for total deaths per year.\n\n# Print table for total deaths per year.\nkable(total_deaths_year, \n      col.names = c(\"Year\", \"Total Deaths\", \"Percentage\"),\n      format = \"html\",\n      digits = 2,\n      caption = \"Total Deaths and Percentage by Year\") %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  scroll_box(height = \"200px\")\n\n\n\nTotal Deaths and Percentage by Year\n\n\nYear\nTotal Deaths\nPercentage\n\n\n\n\n2020\n6326855\n33.62\n\n\n2021\n6544637\n34.78\n\n\n2022\n5947286\n31.60\n\n\n\n\n\n\n\n\nGrouping by month, year, and cause of death, calculating total deaths per year per month, and calculating the percentage of total deaths each year and month. This is for graphing purposes.\n\n# Group by year, month, and cause of death\ncause_counts_year_month &lt;- cause_of_death_data_clean %&gt;%\n  select(-c(`All Cause`, `Number Of Days`)) %&gt;%\n  gather(key = \"Cause of Death\", value = \"count\", -`Jurisdiction of Occurrence`, -Year, -Month) %&gt;%\n  group_by(Year, Month, `Cause of Death`) %&gt;%\n  summarize(total_count = sum(count, na.rm = TRUE)) %&gt;%\n  arrange(Year, Month, desc(total_count))\n\n# Calculate total deaths for each year and month\ntotal_deaths_year_month &lt;- cause_counts_year_month %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(total_deaths = sum(total_count))\n\n# Calculate percentage of total deaths for each year and month\ntotal_deaths_year_month &lt;- total_deaths_year_month %&gt;%\n  mutate(percentage = total_deaths / sum(total_deaths) * 100)\n\nPlot stacked bar plot for causes of death per month per year.\n\n# Create a stacked bar plot for causes of death by month and year\nggplot(cause_counts_year_month, aes(x = Month, y = total_count/1e6, fill = `Cause of Death`)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~Year) +  # facet by year\n  scale_y_continuous(labels = function(x) paste0(format(x, big.mark = \",\", scientific = FALSE), \" million\"), \n                     breaks = pretty_breaks()) + # format y-axis labels\n  labs(title = \"Total Causes of Death by Month & Year\",\n       x = \"Month\",\n       y = \"Total Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nPrint table for total deaths per month per year.\n\n# Print table for total deaths per month per year.\nkable(total_deaths_year_month, \n      col.names = c(\"Year\", \"Month\", \"Total Deaths\", \"Percentage\"),\n      format = \"html\",\n      digits = 2,\n      caption = \"Total Deaths and Percentage by Year and Month\") %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  scroll_box(height = \"200px\")\n\n\n\nTotal Deaths and Percentage by Year and Month\n\n\nYear\nMonth\nTotal Deaths\nPercentage\n\n\n\n\n2020\n1\n458159\n7.24\n\n\n2020\n2\n424927\n6.72\n\n\n2020\n3\n476820\n7.54\n\n\n2020\n4\n640048\n10.12\n\n\n2020\n5\n532282\n8.41\n\n\n2020\n6\n453854\n7.17\n\n\n2020\n7\n519117\n8.20\n\n\n2020\n8\n513981\n8.12\n\n\n2020\n9\n465822\n7.36\n\n\n2020\n10\n501311\n7.92\n\n\n2020\n11\n585911\n9.26\n\n\n2020\n12\n754623\n11.93\n\n\n2021\n1\n774913\n11.84\n\n\n2021\n2\n545362\n8.33\n\n\n2021\n3\n495046\n7.56\n\n\n2021\n4\n466178\n7.12\n\n\n2021\n5\n462739\n7.07\n\n\n2021\n6\n431520\n6.59\n\n\n2021\n7\n457288\n6.99\n\n\n2021\n8\n584022\n8.92\n\n\n2021\n9\n616758\n9.42\n\n\n2021\n10\n568395\n8.68\n\n\n2021\n11\n535910\n8.19\n\n\n2021\n12\n606506\n9.27\n\n\n2022\n1\n740641\n12.45\n\n\n2022\n2\n560742\n9.43\n\n\n2022\n3\n480536\n8.08\n\n\n2022\n4\n431426\n7.25\n\n\n2022\n5\n446988\n7.52\n\n\n2022\n6\n438853\n7.38\n\n\n2022\n7\n463851\n7.80\n\n\n2022\n8\n462356\n7.77\n\n\n2022\n9\n444777\n7.48\n\n\n2022\n10\n467555\n7.86\n\n\n2022\n11\n473950\n7.97\n\n\n2022\n12\n535611\n9.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStashed changes"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Background\nI’m Liza and I’m a first year non-thesis MS student in Bioinformatics. I earned my bachelor’s degree in Biology from Georgia Southern University in 2023. I’m not currently conducting any research, but I’m interested in forensics, microbiology, and infectious disease.\n\n\n\nExperience\nI am somewhat familiar with programming and data analysis. While my knowledge is far from extensive, I have encountered procedural and object based coding languages, and various forms of data analysis in my studies thus far. In this course I am hoping to add to and refine my current skill set in programming and data analysis.\n\n\n\nInterests & Fun Facts\nMy other interests include music, art, cooking, and Star Trek. Fun fact about me, I played trumpet in marching band during my undergrad. See below! :]"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "load dslabs package\nlibrary(dslabs) \nget help file for gapminder data\nhelp(gapminder) \nget overview of data structue\nstr(gapminder) \nget data summary\nsummary(gapminder) \ndetermine gapminder object type\nclass(gapminder)\n\nassign African countries to africadata\nsubset(gapminder, - creates a subset of gapminder dataframe\ncontinent == “Africa”) - filters data to only include African countries\nafricadata &lt;- subset(gapminder, continent == \"Africa\")\n\nreview data\nstr(africadata) - outputs info about africadata\nsummary(africadata) - outputs stats summary for africadata\nstr(africadata) \nsummary(africadata)\n\ncreate new objects\ninfmortality_lifeexpec &lt;-… - assigns results of operation to new variable\nafricadata[, c…] - subsets specific columns from data frame africadata\n(“infant_mortality”, “life_expectancy”) - specifies columns\ninfmortality_lifeexpec &lt;- africadata[, c(\"infant_mortality\", \"life_expectancy\")]\npopulation_lifeexpec &lt;-… - assigns results of operation to new variable\nafricadata[, c…] - subsets specific columns from data frame africadata\n(“population”, “life_expectancy”) - specifies columns\npopulation_lifeexpec &lt;- africadata[, c(\"population\", \"life_expectancy\")]\n\nreview data\nstr(infmortality_lifeexpec) - outputs info about infmortality_lifeexpec\nsummary(infmortality_lifeexpec) - outputs stats summary for infmortality_lifeexpec\nstr(infmortality_lifeexpec) \nsummary(infmortality_lifeexpec)\nstr(population_lifeexpec) - outputs info about population_lifeexpec\nsummary(population_lifeexpec) - outputs stats summary for population_lifeexpec\nstr(population_lifeexpec) \nsummary(population_lifeexpec)\n\nplot data\nplot(…) - creates plot\n(infmortality_lifeexpecSinfant_mortality,…) - takes infant_mortality data from variable infmortality_lifeexpec\n(…,infmortality_lifeexpecSlife_expectancy) - takes life_expectancy data from variable infmortality_lifeexpec\nmain - title of graph\nxlab - x axis title\nylab - y axis title\nplot(infmortality_lifeexpec$infant_mortality, infmortality_lifeexpec$life_expectancy, \n  main = \"Life Expectancy vs. Infant Morality\", \n  xlab = \"Infant Mortality\", \n  ylab = \"Life Expectancy\")\n\nplot(…) - creates plot\n(log10(population_lifeexpecSpopulation),…) - takes population data from variable population_lifeexpec, log scales data\n# (…,infmortality_lifeexpecSlife_expectancy) - takes life_expectancy data from variable infmortality_lifeexpec\n# main - title of graph\n# xlab - x axis title\n# ylab - y axis title\nplot(log10(population_lifeexpec$population), infmortality_lifeexpec$life_expectancy, \n  main = \"Life Expectancy vs. Population\", \n  xlab = \"Population (logscale)\", \n  ylab = \"Life Expectancy\")\n\n\nthere is a negative correlation between infant mortality and life expectancy.\nthere is a positive correlation between population size and life expectancy, as the population grows people live longer.\n\n\nfinding missing data\nmissing_data &lt;-… - assigns results of operation to new variable\nunique(africadataSyear[is.na(africadata$infant_mortality)]) - retreives uniquie data from african data for year only where na is true\nstr(missing_data) - outputs info about missing_data\nmissing_data &lt;- unique(africadata$year[is.na(africadata$infant_mortality)]) \nstr(missing_data)\n\nsingle year\ncreates a subset of africadata where the year = 2000\nafricadata_y2000 &lt;- subset(africadata, year == 2000)\nstr(africadata_y2000) - outputs info about africadata_y2000\nsummary(africadata_y20000) - outputs stats summary for africadata_y2000\nstr(africadata_y2000) \nsummary(africadata_y2000)\n\nplotting\nthis section follows above steps for variable creation and graphing but with data from africadata_y2000\ninfmortality_lifeexpec2000 &lt;- africadata_y2000[, c(\"infant_mortality\", \"life_expectancy\")] population_lifeexpec2000 &lt;- africadata_y2000[, c(\"population\", \"life_expectancy\")]\n\nplot(infmortality_lifeexpec2000Sinfant_mortality, infmortality_lifeexpec2000$life_expectancy,\n  main = \"Life Expectancy vs. Infant Morality\", \n  xlab = \"Infant Mortality\", \n  ylab = \"Life Expectancy\", \n  sub = \"Year 2000\")\n\nplot(log10(population_lifeexpec2000Spopulation), infmortality_lifeexpec2000$life_expectancy,\n  main = \"Life Expectancy vs. Population\", \n  xlab = \"Population (logscale)\", \n  ylab = \"Life Expectancy\", \n  sub = \"Year 2000\")\n\n\nsimple model fits\nfits model with infant mortality as predictor pulling data from africadata_y2000\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africadata_y2000) \nfits model with population size as predictor pulling data from africadata_y2000\nfit2 &lt;- lm(life_expectancy ~ population, data = africadata_y2000)\nsummary(fit1) - outputs stats summary for fit 1\nsummary(fit2) - outputs stats summary for fit 2\nsummary(fit1) \nsummary(fit2)\n\nbased on the p-values for each fit, fit 1 is a better model for the data.\n\n\nThis section is contributed by Ranni Tewfik.\nThis exercise uses the “us_contagious_diseases” dataset from the “dslabs” package. The dataset provides information on contagious diseases in the U.S. by state, year, and disease. There are six variables in the dataset: disease, state, year, weeks reporting, count, and population.\nPart 1 - Loading and Checking Data\n\n#Load the required packages\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n#Look at help file for \"us_contagious_diseases\"\nhelp(us_contagious_diseases)\n\n#Get an overview of data structure\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n#Get a summary of data\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\n#Determine the type of object \"us_contagious_diseases\" is\nclass(us_contagious_diseases)\n\n[1] \"data.frame\"\n\n\nPart 2 - Processing Data\n\n#Assign only Pertussis and Georgia in \"us_contagious_diseases\" to a new object\ngeorgia &lt;- subset(us_contagious_diseases, disease == \"Pertussis\" & state == \"Georgia\")\n\n#Get an overview of data structure and data summary for \"georgia\"\nstr(georgia)\n\n'data.frame':   56 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 11 11 11 11 11 11 11 11 11 11 ...\n $ year           : num  1938 1939 1940 1941 1942 ...\n $ weeks_reporting: num  51 52 51 51 51 52 52 51 50 50 ...\n $ count          : num  1922 1573 1114 1265 1288 ...\n $ population     : num  3066678 3095013 3123723 3152430 3181234 ...\n\nsummary(georgia)\n\n        disease          state         year      weeks_reporting\n Hepatitis A: 0   Georgia   :56   Min.   :1938   Min.   : 2.00  \n Measles    : 0   Alabama   : 0   1st Qu.:1952   1st Qu.:39.00  \n Mumps      : 0   Alaska    : 0   Median :1984   Median :46.50  \n Pertussis  :56   Arizona   : 0   Mean   :1978   Mean   :41.75  \n Polio      : 0   Arkansas  : 0   3rd Qu.:1997   3rd Qu.:49.25  \n Rubella    : 0   California: 0   Max.   :2011   Max.   :52.00  \n Smallpox   : 0   (Other)   : 0                                 \n     count          population     \n Min.   :   2.0   Min.   :3066678  \n 1st Qu.:  27.0   1st Qu.:3518196  \n Median :  58.0   Median :5776280  \n Mean   : 346.0   Mean   :5898795  \n 3rd Qu.: 571.5   3rd Qu.:7688876  \n Max.   :1922.0   Max.   :9830160  \n                                   \n\n#Create a new object that only contains year and count\ngeorgia1 &lt;- georgia %&gt;% select(\"year\", \"count\")\n  \n#Create a new object that only contains population and count\ngeorgia2 &lt;- georgia %&gt;% select(\"population\", \"count\")\n\n#Get an overview of data structure and data summary for \"georgia1\" and \"georgia2\"\nstr(georgia1)\n\n'data.frame':   56 obs. of  2 variables:\n $ year : num  1938 1939 1940 1941 1942 ...\n $ count: num  1922 1573 1114 1265 1288 ...\n\nsummary(georgia1)\n\n      year          count       \n Min.   :1938   Min.   :   2.0  \n 1st Qu.:1952   1st Qu.:  27.0  \n Median :1984   Median :  58.0  \n Mean   :1978   Mean   : 346.0  \n 3rd Qu.:1997   3rd Qu.: 571.5  \n Max.   :2011   Max.   :1922.0  \n\nstr(georgia2)\n\n'data.frame':   56 obs. of  2 variables:\n $ population: num  3066678 3095013 3123723 3152430 3181234 ...\n $ count     : num  1922 1573 1114 1265 1288 ...\n\nsummary(georgia2)\n\n   population          count       \n Min.   :3066678   Min.   :   2.0  \n 1st Qu.:3518196   1st Qu.:  27.0  \n Median :5776280   Median :  58.0  \n Mean   :5898795   Mean   : 346.0  \n 3rd Qu.:7688876   3rd Qu.: 571.5  \n Max.   :9830160   Max.   :1922.0  \n\n\nPart 3 - Plotting\n\n#Plot count as a function of year\nggplot(georgia1, aes(x = year, y = count)) + geom_point() + ggtitle(\"Total Number of Reported Cases of Pertussis in Georgia by Year\")\n\n\n\n\nThere is a negative correlation between year and count to a certain point in the plot, and then there is a break in the data (no available data for 1956-1973). After that point, there seems to be no correlation between year and count.\n\n#Plot count as a function of population\nggplot(georgia2, aes(x = log(population), y = count)) + geom_point() + ggtitle(\"Total Number of Reported Cases of Pertussis in Georgia by Population\")\n\n\n\n\nSimilar to the previous plot, there is a negative correlation between population and count to a certain point. However, there is a noticeable break in the data, and there seems to be no correlation between population and count after that point. This is because no data is available for the years 1956-1973.\nPart 4 - More Data Processing\n\n#Create a new object by extracting only the data for the years after 1973 from \"georgia\"\ngeorgia3 &lt;- georgia[(georgia$year &gt;= 1974),]\n\n\n#Get an overview of data structure and data summary for \"africadata3\"\nstr(georgia3)\n\n'data.frame':   38 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 11 11 11 11 11 11 11 11 11 11 ...\n $ year           : num  1974 1975 1976 1977 1978 ...\n $ weeks_reporting: num  6 7 2 19 33 29 40 27 22 48 ...\n $ count          : num  10 8 2 139 75 81 121 57 39 59 ...\n $ population     : num  4920562 5009127 5099141 5190101 5281471 ...\n\nsummary(georgia3)\n\n        disease          state         year      weeks_reporting\n Hepatitis A: 0   Georgia   :38   Min.   :1974   Min.   : 2.00  \n Measles    : 0   Alabama   : 0   1st Qu.:1983   1st Qu.:34.75  \n Mumps      : 0   Alaska    : 0   Median :1992   Median :42.50  \n Pertussis  :38   Arizona   : 0   Mean   :1992   Mean   :38.45  \n Polio      : 0   Arkansas  : 0   3rd Qu.:2002   3rd Qu.:47.75  \n Rubella    : 0   California: 0   Max.   :2011   Max.   :51.00  \n Smallpox   : 0   (Other)   : 0                                 \n     count          population     \n Min.   :  2.00   Min.   :4920562  \n 1st Qu.: 23.00   1st Qu.:5753480  \n Median : 40.00   Median :6852235  \n Mean   : 49.87   Mean   :7111873  \n 3rd Qu.: 58.50   3rd Qu.:8480435  \n Max.   :146.00   Max.   :9830160  \n                                   \n\n\nPart 5 - More Plotting\n\n#Plot count as a function of year for the years after 1973\nggplot(georgia3, aes(x = year, y = count)) + geom_point() + ggtitle(\"Total Number of Reported Cases of Pertussis in Georgia by Year After 1973\")\n\n\n\n\nThere is no noticeable correlation between year and count after 1973.\n\n#Plot count as a function of population for the years after 1973\nggplot(georgia3, aes(x = log(population), y = count)) + geom_point() + ggtitle(\"Total Number of Reported Cases of Pertussis in Georgia by Population After 1973\")\n\n\n\n\nThere is no noticeable correlation between population and count after 1973.\nPart 6 - Simple Model Fits\n\n#Fit count as the outcome and year as the predictor\nfit1 &lt;- glm(count ~ year, data = georgia3, family = poisson(link = \"log\"))\nsummary(fit1)\n\n\nCall:\nglm(formula = count ~ year, family = poisson(link = \"log\"), data = georgia3)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  8.810681   4.174343   2.111   0.0348 *\nyear        -0.002460   0.002095  -1.174   0.2404  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1026.6  on 37  degrees of freedom\nResidual deviance: 1025.3  on 36  degrees of freedom\nAIC: 1235.8\n\nNumber of Fisher Scoring iterations: 5\n\n\nAfter 1973, count is not significantly associated with year (p-value = 0.24).\n\n#Fit count as the outcome and population as the predictor\nfit2 &lt;- glm(count ~ population, data = georgia3, family = poisson(link = \"log\"))\nsummary(fit2)\n\n\nCall:\nglm(formula = count ~ population, family = poisson(link = \"log\"), \n    data = georgia3)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.007e+00  1.089e-01  36.801   &lt;2e-16 ***\npopulation  -1.370e-08  1.503e-08  -0.912    0.362    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1026.6  on 37  degrees of freedom\nResidual deviance: 1025.8  on 36  degrees of freedom\nAIC: 1236.3\n\nNumber of Fisher Scoring iterations: 5\n\n\nAfter 1973, count is not significantly associated with population (p-value = 0.36)."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "Creating the Dataset\nLoad required packages, and set seed.\n# load required packages\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(httr)\n\n# set seed for reproducibility \nset.seed(234)\nSet number of artists and songs, and create empty dataframe.\n\n# Set the number of songs and artists\nn_songs &lt;- 300\nn_artists &lt;- 20\n\n# Create an empty data frame with placeholders for music-related variables\nmusic_data &lt;- data.frame(\nSongID = 1:n_songs,\nTitle = character(n_songs),\nArtist = character(n_songs),\nGenre = character(n_songs),\nBPM = numeric(n_songs),\nDanceability = numeric(n_songs),\nLengthInSeconds = numeric(n_songs),\nReleaseYear = integer(n_songs),\nRating = numeric(n_songs)\n)\n\nCreate a vector of artists names, adjectives, and nouns. This will be used to assign an artist name and to generate random song names. (This part was overkill but I thought it was fun.)\n\n# Define a vector of artist names\n# I used a random name generator (https://igenerator.net/random-name-generator/artist-name-generator/) to generate names\n# I also used this random band name generator (https://rocklou.com/bandnamegenerator)\nartist_names &lt;- c(\"Ophelia Onyx\", \"Raina Ruby\", \"Grayson Knight\", \"Atlas Stonehart\", \"Indigo Stone\",\n                  \"Jasper Wolf\", \"Alex Ace\", \"Gary Joy\", \"JTK\", \"Julian Howard\", \n                  \"LLAP\", \"Grey Century\", \"Coven Atmosphere\", \"Eternal Symphonies\", \"The Inner Ethers\",\n                  \"The Whirlwind\", \"H.E.A.V.Y\", \"Orange Vinyl\", \"Days Of Cities\", \"Enterprise\")\n\n# Ensure there are exactly 20 names in the vector\nif(length(artist_names) != 20) {\n  stop(\"The number of artist names should be exactly 20.\")\n}\n  \n# Generating random song titles (probably overkill but I think its fun)\n# I used this random word generator (https://randomwordgenerator.com)\ngenerate_random_title &lt;- function() {\n\n  adjectives &lt;- c(\"Lame\", \"Towering\", \"Ambiguous\", \"Vagabond\", \"Limping\",\n                  \"Quarrelsome\", \"Guiltless\", \"Vengeful\", \"Quirky\", \"Chief\",\n                  \"Worried\", \"Simple\", \"Redundant\", \"Charming\", \"Jobless\",\n                  \"Impossible\", \"False\", \"Significant\", \"Secretive\", \"Lackadaisical\",\n                  \"Alive\", \"Disillusioned\", \"Thin\", \"Legal\", \"Unhappy\",\n                  \"Dangerous\", \"Vivacious\", \"Important\", \"Wistful\", \"Condemned\")\n  \n       nouns &lt;- c(\"Penalty\", \"Camera\", \"Leader\", \"Membership\", \"Oven\",\n                  \"Promotion\", \"Wealth\", \"Lake\", \"Drama\", \"Map\",\n                  \"Cabinet\", \"Quality\", \"Television\", \"Guest\", \"Bath\",\n                  \"Bonus\", \"Soup\", \"Agency\", \"Insect\", \"Person\",\n                  \"Student\", \"Nation\", \"Manufacturer\", \"Player\", \"Year\",\n                  \"Consequence\", \"Meal\", \"Presence\", \"Environment\", \"Criticism\")\n  \n  random_adjective &lt;- sample(adjectives, 1)\n  random_noun &lt;- sample(nouns, 1)\n  title &lt;- paste(random_adjective, random_noun, sep = \" \")\n  return(title)\n}\n\nGenerating synthetic data.\n\n# Generate synthetic data for music-related variables\nmusic_data$Title &lt;- replicate(n_songs, generate_random_title())\nmusic_data$Artist &lt;- sample(artist_names, n_songs, replace = TRUE)\nmusic_data$Genre &lt;- sample(c(\"Pop\", \"Rock\", \"Hip-Hop\", \"Electronic\", \"Jazz\"), n_songs, replace = TRUE)\nmusic_data$BPM &lt;- round(rnorm(n_songs, mean = 120, sd = 20))\nmusic_data$Danceability &lt;- round(pmax(pmin(runif(n_songs, min = 1, max = 10), 10), 1))\nmusic_data$LengthInSeconds &lt;- round(rnorm(n_songs, mean = 240, sd = 30))\nmusic_data$ReleaseYear &lt;- sample(1950:2020, n_songs, replace = TRUE)\nmusic_data$Rating &lt;- round(pmax(pmin(rnorm(n_songs, mean = 3.5, sd = 1), 10), 1))\n\n\n\nIntroducing Dependencies Between Variables\nAssigning artists a primary genre.\n\n# Artists produce songs in a primary genre, but occasionally produce songs from other genres\nartists &lt;- paste0(\"Artist\", 1:n_artists)\nprimary_genres &lt;- sample(c(\"Pop\", \"Rock\", \"Hip-Hop\", \"Electronic\", \"Jazz\"), n_artists, replace = TRUE)\nartist_genre_mapping &lt;- data.frame(Artist = artists, PrimaryGenre = primary_genres)\nfraction_other_genres &lt;- 0.2\nfor (i in 1:n_artists) {\n  artist_songs &lt;- music_data$Artist == artists[i]\n  primary_genre &lt;- artist_genre_mapping$PrimaryGenre[i]\n  if (any(artist_songs)) {\n    num_songs_total &lt;- sum(artist_songs)\n    num_other_genres &lt;- round(num_songs_total * fraction_other_genres)\n    other_indices &lt;- sample(which(artist_songs), num_other_genres)\n    music_data$Genre[other_indices] &lt;- sample(setdiff(c(\"Pop\", \"Rock\", \"Hip-Hop\", \"Electronic\", \"Jazz\"), primary_genre), num_other_genres, replace = TRUE)\n    music_data$Genre[setdiff(which(artist_songs), other_indices)] &lt;- primary_genre\n  }\n}\n\nGenre effects average BPM.\n\n# Songs in Jazz and Pop genres tend to have higher BPMs\nfraction_adjust_bpm_jazz_pop &lt;- 0.8 \njazz_pop_indices &lt;- which(music_data$Genre %in% c(\"Jazz\", \"Pop\") & runif(n_songs) &lt; fraction_adjust_bpm_jazz_pop)\nmusic_data$BPM[jazz_pop_indices] &lt;- music_data$BPM[jazz_pop_indices] + 20\n\n# Songs in the Rock and Hip-Hop genres tend to have lower BPMs\nfraction_adjust_bpm_hip_hop &lt;- 0.8\nhip_hop_indices &lt;- which(music_data$Genre %in% c(\"Hip-Hop\") & runif(n_songs) &lt; fraction_adjust_bpm_hip_hop)\nmusic_data$BPM[hip_hop_indices] &lt;- music_data$BPM[hip_hop_indices] - 10\nfraction_adjust_bpm_rock &lt;- 0.6\nrock_indices &lt;- which(music_data$Genre %in% c(\"Rock\") & runif(n_songs) &lt; fraction_adjust_bpm_rock)\nmusic_data$BPM[rock_indices] &lt;- music_data$BPM[rock_indices] - 10\n\nDanceability effects ratings. BPM range effects danceability.\n\n# Songs with higher danceability tend to have higher ratings\nmusic_data$Rating[music_data$Danceability &gt; 5] &lt;- music_data$Rating[music_data$Danceability &gt; 5] + 3\n\n# Songs between 115-125 BPM get a boost in danceability\nbpm_boost_indices &lt;- which(music_data$BPM &gt;= 115 & music_data$BPM &lt;= 125)\nmusic_data$Danceability[bpm_boost_indices] &lt;- music_data$Danceability[bpm_boost_indices] + 3\n\nGenre effects release year.\n\n# Songs in the Rock and Jazz genres tend to be older\nfraction_replace_jazz &lt;- 0.7  \nfraction_replace_rock &lt;- 0.6  \njazz_indices &lt;- which(music_data$Genre %in% c(\"Jazz\") & runif(n_songs) &lt; fraction_replace_jazz)\nrock_indices &lt;- which(music_data$Genre %in% c(\"Rock\") & runif(n_songs) &lt; fraction_replace_rock)\nmusic_data$ReleaseYear[jazz_indices] &lt;- sample(c(1950:1970), length(jazz_indices), replace = TRUE)\nmusic_data$ReleaseYear[rock_indices] &lt;- sample(c(1970:2000), length(rock_indices), replace = TRUE)\n\n# Songs in the Pop and Electronic genres tend to be newer\nfraction_replace_electronic &lt;- 0.6  \nfraction_replace_pop &lt;- 0.5  \nelectronic_indices &lt;- which(music_data$Genre %in% c(\"Electronic\") & runif(n_songs) &lt; fraction_replace_electronic)\npop_indices &lt;- which(music_data$Genre %in% c(\"Pop\") & runif(n_songs) &lt; fraction_replace_pop)\nmusic_data$ReleaseYear[electronic_indices] &lt;- sample(c(1990:2010), length(electronic_indices), replace = TRUE)\nmusic_data$ReleaseYear[pop_indices] &lt;- sample(c(2000:2020), length(pop_indices), replace = TRUE)\n\n\n\nAnalyzing and Displaying the Data\nGeneral summary of data.\n\n# Display the first few rows of the synthetic music dataset\nhead(music_data)\n\n  SongID                  Title           Artist      Genre BPM Danceability\n1      1            Lame Camera      Jasper Wolf        Pop 167            6\n2      2        Charming Camera      Jasper Wolf       Rock 129            4\n3      3 Significant Television The Inner Ethers       Rock 100            8\n4      4          Legal Penalty The Inner Ethers    Hip-Hop 137            2\n5      5    Vagabond Membership     Indigo Stone Electronic 166            5\n6      6            Thin Nation              JTK       Rock 123            4\n  LengthInSeconds ReleaseYear Rating\n1             228        2019      6\n2             213        1959      4\n3             245        1979      7\n4             238        1971      4\n5             226        2009      4\n6             264        1970      4\n\n\n\nBar plot for genre vs BPM. This is to show the correlation between genre and BPM.\n\n# Bar plot for Genre vs BPM\nggplot(music_data, aes(x = Genre, y = BPM, fill = Genre)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Genre vs BPM in Synthetic Music Dataset\",\n       x = \"Genre\",\n       y = \"BPM\") +\n  theme_minimal()\n\n\n\n\nScatter plot for BPM and danceability. This is to show the correlation between BPM and danceability.\n\n# Scatter plot for BPM and Danceability\nggplot(music_data, aes(x = BPM, y = Danceability, color = Genre)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of BPM and Danceability\",\n       x = \"BPM\",\n       y = \"Danceability\",\n       color = \"Genre\") +\n  theme_minimal()\n\n\n\n\nBoxplot for ratings across genres.\n\n# Boxplot for Ratings across Genres\nggplot(music_data, aes(x = Genre, y = Rating, fill = Genre)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Ratings Across Genres\",\n       x = \"Genre\",\n       y = \"Rating\",\n       fill = \"Genre\") +\n  theme_minimal()\n\n\n\n\nBoxplot for genres across years. This is to show how genre affects release year.\n\n# Boxplot for Genres across Years\nggplot(music_data, aes(x = Genre, y = ReleaseYear, fill = Genre)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Genres Across Years\",\n       x = \"Genre\",\n       y = \"Release Year\",\n       fill = \"Genre\") +\n  theme_minimal()\n\n\n\n\nBar plot for danceability vs ratings. This is to show the relationship between ratings and danceability.\n\n# Bar plot for Danceability vs Ratings\nggplot(music_data, aes(x = Danceability, y = Rating, fill = factor(Danceability))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Bar Graph of Danceability vs Ratings\",\n       x = \"Danceability\",\n       y = \"Rating\") +\n  theme_minimal()\n\n\n\n\nBar plot for selected artists vs genre. This is to show that artists were assigned a primary genre.\n\n# Selecting only half of the artists for better visibility\nselected_artists &lt;- sample(artist_names, n_artists / 2)\n\n# Bar plot for Selected Artists vs Genre\nggplot(music_data[music_data$Artist %in% selected_artists, ], \n       aes(x = Artist, fill = Genre)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Bar Graph of Selected Artists vs Genre of Songs Produced\",\n       x = \"Artist\",\n       y = \"Count\",\n       fill = \"Genre\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nTable of the top rated songs, just for fun.\n\n# Creating a data frame with top-rated songs\ntop_rated_songs &lt;- music_data[order(music_data$Rating, decreasing = TRUE), ]\n\n# Selecting the top 10 rated songs (you can adjust the number as needed)\ntop_rated_songs &lt;- head(top_rated_songs, 10)\n\n# Displaying the table with Song Title, Artist, Genre, and Rating\ntop_rated_songs_table &lt;- data.frame(Song_Title = top_rated_songs$Title,\n                                    Artist = top_rated_songs$Artist,\n                                    Genre = top_rated_songs$Genre,\n                                    Rating = top_rated_songs$Rating)\n\nprint(top_rated_songs_table)\n\n           Song_Title             Artist      Genre Rating\n1   Impossible Player         Enterprise Electronic      9\n2       Jobless Guest Eternal Symphonies       Jazz      8\n3    Ambiguous Insect        Jasper Wolf    Hip-Hop      8\n4    Secretive Wealth         Raina Ruby       Jazz      8\n5        Thin Penalty   The Inner Ethers       Rock      8\n6  Redundant Presence Eternal Symphonies        Pop      8\n7         Thin Wealth      Julian Howard Electronic      8\n8    Lame Consequence           Gary Joy       Rock      8\n9    Dangerous Leader        Jasper Wolf    Hip-Hop      8\n10   Worried Presence           Alex Ace        Pop      8\n\n\n\nLinear model for rating with BPM and danceability as predictors.\nThis produces a p-value of 2.2e-16 which indicates signifigance.\n\n# Fit a linear model for Rating with BPM and Danceability as predictors.\nlinear_model &lt;- lm(Rating ~ BPM + Danceability, data = music_data)\nsummary(linear_model)\n\n\nCall:\nlm(formula = Rating ~ BPM + Danceability, data = music_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7677 -1.0787 -0.0488  0.9247  3.0917 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.4461375  0.4444838   5.503 8.06e-08 ***\nBPM          -0.0003266  0.0032833  -0.099    0.921    \nDanceability  0.4198899  0.0280700  14.959  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.378 on 297 degrees of freedom\nMultiple R-squared:  0.4298,    Adjusted R-squared:  0.4259 \nF-statistic: 111.9 on 2 and 297 DF,  p-value: &lt; 2.2e-16\n\n\n\nLinear model for BPM with genre as a predictor.\nThis produces a p-value of 2.2e-16 which indicates signifigance.\n\n# Fit a linear model for BPM with Genre as a predictor.\nlinear_model_rating_interaction &lt;- lm(BPM ~ Genre, data = music_data)\nsummary(linear_model_rating_interaction)\n\n\nCall:\nlm(formula = BPM ~ Genre, data = music_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.138 -13.686   0.662  14.896  55.325 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   127.000      2.691  47.193  &lt; 2e-16 ***\nGenreHip-Hop  -17.542      4.037  -4.346 1.91e-05 ***\nGenreJazz      11.138      3.732   2.985  0.00308 ** \nGenrePop       11.720      3.991   2.936  0.00358 ** \nGenreRock     -16.325      3.590  -4.548 7.92e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.84 on 295 degrees of freedom\nMultiple R-squared:  0.2731,    Adjusted R-squared:  0.2632 \nF-statistic:  27.7 on 4 and 295 DF,  p-value: &lt; 2.2e-16\n\n\n\nLinear model for release year with genre as a predictor.\nThis produces a p-value of 2.2e-16 which indicates signifigance.\n\n# Fit a linear model for Release Year with Genre as a predictor.\nlinear_model_genre_year &lt;- lm(ReleaseYear ~ Genre, data = music_data)\nsummary(linear_model_genre_year)\n\n\nCall:\nlm(formula = ReleaseYear ~ Genre, data = music_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.617 -11.558  -0.477  11.900  48.523 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1994.617      2.390 834.638  &lt; 2e-16 ***\nGenreHip-Hop   -8.992      3.585  -2.508   0.0127 *  \nGenreJazz     -28.140      3.314  -8.491 1.02e-15 ***\nGenrePop        3.483      3.545   0.983   0.3266    \nGenreRock      -8.058      3.188  -2.528   0.0120 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.51 on 295 degrees of freedom\nMultiple R-squared:  0.2655,    Adjusted R-squared:  0.2555 \nF-statistic: 26.65 on 4 and 295 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Welcome! :]\nHello! This is my website and data analysis portfolio.\nUse the Menu Bar above to look around."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "The first step was to import and read the data from the csv file.\nFirst, the required libraries ‘here’ and ‘readr’ were loaded.\n\n# Load here and readr for data reading\nlibrary(here)\nlibrary(readr)\n\n\n\nNext, the data was read from the csv file.\n\n# Use the here function to specify the file path\ndata_path &lt;- here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\")\n\n# Load the data\ndata &lt;- read_csv(data_path)\n\n\nPrinting the first few rows of the dataset, to make sure it loaded correctly.\n\n# Inspect the first few rows of the data to ensure it's loaded correctly\nhead(data)\n\n# A tibble: 6 × 17\n     ID   CMT  EVID  EVI2   MDV    DV  LNDV   AMT  TIME  DOSE   OCC  RATE   AGE\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   793     1     1     1     1     0  0       25 0        25     1    75    42\n2   793     2     0     0     0   491  6.20     0 0.2      25     1     0    42\n3   793     2     0     0     0   605  6.40     0 0.25     25     1     0    42\n4   793     2     0     0     0   556  6.32     0 0.367    25     1     0    42\n5   793     2     0     0     0   310  5.74     0 0.533    25     1     0    42\n6   793     2     0     0     0   237  5.47     0 0.7      25     1     0    42\n# ℹ 4 more variables: SEX &lt;dbl&gt;, RACE &lt;dbl&gt;, WT &lt;dbl&gt;, HT &lt;dbl&gt;\n\n\n\nLoading in ggplot2 for plotting.\nPlotting ‘DV’ as a function of time, stratified by ‘DOSE’ with ‘ID’ as a grouping factor. This provides a quick visual overview of the data.\n\n# Load ggplot2 for plotting\nlibrary(ggplot2)\n\n# Define colors\ncolors &lt;- c(\"#5c88da\", \"#84bd00\", \"#ffcd00\")\n\nggplot(data, aes(x = TIME, y = DV, group = ID, color = as.factor(DOSE))) +\n  geom_line() +\n  scale_color_manual(values = colors) +  \n  facet_wrap(~ DOSE, scales = \"free\", ncol = 1) +  \n  labs(title = \"DV vs. Time Stratified by DOSE\", x = \"Time\", y = \"DV\", color = \"DOSE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLoading in dplyr for data manipulation.\n\n# Load dplyr for data malipulation\nlibrary(dplyr)\n\n\nFirst, data is filtered to keep only rows where ‘OOC’ is equal to 1.\nThen the first few rows of the dataframe are printed, to ensure that the data was filtered correctly.\n\n# Filter the DataFrame to keep only rows where OCC equals 1\ndata1 &lt;- subset(data, OCC == 1)\n\n# Print the first few lines of the filtered dataframe\nprint(head(data1))\n\n# A tibble: 6 × 17\n     ID   CMT  EVID  EVI2   MDV    DV  LNDV   AMT  TIME  DOSE   OCC  RATE   AGE\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   793     1     1     1     1     0  0       25 0        25     1    75    42\n2   793     2     0     0     0   491  6.20     0 0.2      25     1     0    42\n3   793     2     0     0     0   605  6.40     0 0.25     25     1     0    42\n4   793     2     0     0     0   556  6.32     0 0.367    25     1     0    42\n5   793     2     0     0     0   310  5.74     0 0.533    25     1     0    42\n6   793     2     0     0     0   237  5.47     0 0.7      25     1     0    42\n# ℹ 4 more variables: SEX &lt;dbl&gt;, RACE &lt;dbl&gt;, WT &lt;dbl&gt;, HT &lt;dbl&gt;\n\n\n\nNext, any observations where ‘TIME’ equals 0 are filtered out.\nThen the sum of the DV variables are computed and assigned to variable Y.\nThen a dataframe is created, which only contains observations where ‘TIME’ equals 0.\nFinally, ‘inner_join’ is used to combine the dataframes.\n\n# Filter out observations where TIME is not equal to 0\ndata_filtered &lt;- filter(data1, TIME != 0)\n\n# Compute the sum of the DV variable for each individual\nY &lt;- data_filtered %&gt;%\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV))\n\n# Create a dataframe containing only the observations where TIME equals 0\ndata_TIME_0 &lt;- filter(data1, TIME == 0)\n\n# Combine the two dataframes using the appropriate join function\ncombined_data &lt;- inner_join(Y, data_TIME_0, by = \"ID\")\n\n\nNext, ‘RACE’ and ‘SEX’ are converted to factor variables.\nThen variables ‘Y’, ‘DOSE’, ‘AGE’, ‘SEX’, ‘RACE’, ‘WT’, and ‘HT’ are assigned to selected_data.\n\n# Convert RACE and SEX to factor variables\ncombined_data &lt;- combined_data %&gt;%\n  mutate(RACE = factor(RACE),\n         SEX = factor(SEX))\n\n# Select only the desired variables\nselected_data &lt;- combined_data %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT)\n\n\nSaving ‘selected_data’ to RDs file for use in module 11 exercise\n\n# Save cleaned data to RDs file for use in Exercise 11\nRDSfilepath &lt;- here(\"ml-models-exercise\", \"cleandata.rds\")\nwrite_rds(selected_data, RDSfilepath)\n\n\n\n\n\n\n\nLoading in packages for data exploration and visualization.\n\n# Load tidyr, knitr, ggsci, and corrplot for data exploration and visualization\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggsci)\nlibrary(corrplot)\n\n\nTo get a better idea of the variables, summary table is printed.\n\n# Summary table for all variables\nsummary_data &lt;- summary(selected_data[, c(\"Y\", \"DOSE\", \"AGE\", \"SEX\", \"RACE\", \"WT\", \"HT\")])\nprint(summary_data)\n\n       Y               DOSE            AGE        SEX     RACE   \n Min.   : 826.4   Min.   :25.00   Min.   :18.00   1:104   1 :74  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:26.00   2: 16   2 :36  \n Median :2349.1   Median :37.50   Median :31.00           7 : 2  \n Mean   :2445.4   Mean   :36.46   Mean   :33.00           88: 8  \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:40.25                  \n Max.   :5606.6   Max.   :50.00   Max.   :50.00                  \n       WT               HT       \n Min.   : 56.60   Min.   :1.520  \n 1st Qu.: 73.17   1st Qu.:1.700  \n Median : 82.10   Median :1.770  \n Mean   : 82.55   Mean   :1.759  \n 3rd Qu.: 90.10   3rd Qu.:1.813  \n Max.   :115.30   Max.   :1.930  \n\n\n\nCreating a scatterplot for ‘Y’ vs ‘DOSE’.\n\n# Define colors\ncolors &lt;- c(\"#cc1c00\", \"#5c88da\", \"#84bd00\", \"#ffcd00\")\n\n# Create the scatterplot of Y vs. DOSE \nggplot(selected_data, aes(x = DOSE, y = Y)) +\n  geom_point(aes(color = DOSE), alpha = 0.6, size = 3) +\n  scale_color_gradientn(colors = colors) +\n  labs(title = \"Y vs. DOSE\", x = \"Dose\", y = \"Y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 20),\n        axis.title.x = element_text(face=\"bold\", size = 14),\n        axis.title.y = element_text(face=\"bold\", size = 14),\n        legend.title = element_blank())\n\n\n\n\n\nCreating a scatterplot for ‘Y’ vs ‘AGE’.\n\n# Scatterplot of Y vs. AGE\nggplot(selected_data, aes(x = AGE, y = Y)) +\n  geom_point(aes(color = AGE), alpha = 0.6, size = 3) +\n  scale_color_gradient(low = \"#5c88da\", high = \"#ffcd00\") + \n  labs(title = \"Y vs. AGE\", x = \"Age\", y = \"Y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 20),\n        axis.title.x = element_text(face=\"bold\", size = 14),\n        axis.title.y = element_text(face=\"bold\", size = 14),\n        legend.title = element_text(size = 14))\n\n\n\n\n\nCreating a boxplot for ‘Y’ vs ‘SEX’.\n\n# Boxplot of Y vs. SEX \nggplot(selected_data, aes(x = as.factor(SEX), y = Y, fill = as.factor(SEX))) +\n  geom_boxplot() +\n  scale_fill_startrek() + \n  labs(title = \"Y vs. SEX\", x = \"Sex\", y = \"Y\") +\n  theme_minimal() +\n  theme(legend.title = element_blank(), \n        plot.title = element_text(hjust = 0.5),\n        axis.title.x = element_text(face=\"bold\"),\n        axis.title.y = element_text(face=\"bold\"))\n\n\n\n\n\nCreating a boxplot for ‘Y’ vs ‘RACE’.\n\n# Boxplot of Y vs. RACE\nggplot(selected_data, aes(x = as.factor(RACE), y = Y, fill = as.factor(RACE))) +\n  geom_boxplot() +\n  scale_fill_startrek() + \n  labs(title = \"Y vs. RACE\", x = \"Race\", y = \"Y\") +\n  theme_minimal() +\n  theme(legend.title = element_blank(), \n        plot.title = element_text(hjust = 0.5),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n        axis.title.x = element_text(face=\"bold\"),\n        axis.title.y = element_text(face=\"bold\"))\n\n\n\n\n\nCreating distribution curves for ‘WT’ and ‘HT’.\n\n# Distribution of WT with density curve\nggplot(selected_data, aes(x = WT)) + \n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = \"#5c88da\", color = \"black\") + \n  geom_density(alpha = 0.5, color = \"black\") + \n  theme_minimal() + \n  ggtitle(\"Distribution of WT\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n# Distribution of HT with density curve\nggplot(data, aes(x = HT)) + \n  geom_histogram(aes(y = ..density..), fill = \"#ffcd00\", color = \"black\") + \n  geom_density(alpha = 0.5, color = \"black\") + \n  theme_minimal() + \n  ggtitle(\"Distribution of HT\")\n\n\n\n\n\nPlotting correlation matrix.\n\n# Convert variables_of_interest to numeric\nvariables_of_interest &lt;- as.data.frame(sapply(selected_data, as.numeric))\n\n# Calculate correlation matrix for the variables of interest\ncorr_matrix &lt;- cor(variables_of_interest, use = \"complete.obs\")\n\n# Your custom colors\nmy_colors &lt;- colorRampPalette(c(\"#5c88da\", \"white\", \"#ffcd00\"))(200) \n\n# Plotting the correlation matrix with custom colors\ncorrplot(corr_matrix, method = \"color\", col = my_colors,\n         tl.col=\"black\", tl.srt=45) # Text label color and rotation\n\n\n\n\n\n\n\n\n\n\nLoading in tidymodels for model fitting.\n\n# Load tidymodels for model fitting \nlibrary(tidymodels)\n\n\n\n\n\nModels were fitted to the continuous outcome ‘Y’.\n\nDefining the linear model specification.\n\n# Define the model specification\nlinear_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n\nFitting model with ‘DOSE’ as the predictor.\n\n# Model with DOSE as predictor\n# Fit the model\nmodel_dose &lt;- linear_spec %&gt;% \n  fit(Y ~ DOSE, data = selected_data)\n\n# Summarize the model\nsummary(model_dose$fit)\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1284.91  -441.14   -97.22   325.84  2372.87 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  323.062    199.049   1.623    0.107    \nDOSE          58.213      5.194  11.208   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 672.1 on 118 degrees of freedom\nMultiple R-squared:  0.5156,    Adjusted R-squared:  0.5115 \nF-statistic: 125.6 on 1 and 118 DF,  p-value: &lt; 2.2e-16\n\n\n\nCalculating RMSE and R-squared for ‘DOSE’ model.\n\n# RMSE and R-squared for model with DOSE\nrmse_dose &lt;- model_dose %&gt;% \n  predict(new_data = selected_data) %&gt;% \n  bind_cols(selected_data) %&gt;% \n  metrics(truth = Y, estimate = .pred) %&gt;% \n  filter(.metric %in% c(\"rmse\", \"rsq\"))\n\nprint(rmse_dose)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n\n\n\nFitting model with all predictors.\n\n# Model with all predictors\n# Fit the model\nmodel_all &lt;- linear_spec %&gt;% \n  fit(Y ~ ., data = selected_data)\n\n# Summarize the model\nsummary(model_all$fit)\n\n\nCall:\nstats::lm(formula = Y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1480.86  -367.81   -79.95   266.15  2431.52 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3386.863   1835.153   1.846 0.067623 .  \nDOSE          59.935      4.880  12.281  &lt; 2e-16 ***\nAGE            3.155      7.825   0.403 0.687530    \nSEX2        -357.734    216.928  -1.649 0.101957    \nRACE2        155.034    128.627   1.205 0.230650    \nRACE7       -405.320    448.189  -0.904 0.367768    \nRACE88       -53.505    244.668  -0.219 0.827296    \nWT           -23.047      6.395  -3.604 0.000471 ***\nHT          -748.487   1103.979  -0.678 0.499188    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 614.3 on 111 degrees of freedom\nMultiple R-squared:  0.6193,    Adjusted R-squared:  0.5919 \nF-statistic: 22.57 on 8 and 111 DF,  p-value: &lt; 2.2e-16\n\n\n\nCalculating RMSE and R-squared for all predictors model.\n\n# RMSE and R-squared for model with all predictors\nrmse_all &lt;- model_all %&gt;% \n  predict(new_data = selected_data) %&gt;% \n  bind_cols(selected_data) %&gt;% \n  metrics(truth = Y, estimate = .pred) %&gt;% \n  filter(.metric %in% c(\"rmse\", \"rsq\"))\n\nprint(rmse_all)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     591.   \n2 rsq     standard       0.619\n\n\n\nThe lower RMSE and MAE values for the all predictors model compared to the ‘DOSE’ model indicate better accuracy in predicting Y when all predictors are considered.\nThe higher R-squared value for the all predictors model suggests that it does a better job in explaining variance in the data.\n\n\n\n\n\nModels were fitted to the categorical/binary outcome ‘SEX’.\nDefining the logistic model specification.\n\n# Define the model specification\nlogistic_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\")\n\n\nFitting model with ‘DOSE’ as the predictor.\n\n# Fit the model\nlogistic_dose &lt;- logistic_spec %&gt;% \n  fit(SEX ~ DOSE, data = selected_data)\n\nprint(logistic_dose)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ DOSE, family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n   -0.76482     -0.03175  \n\nDegrees of Freedom: 119 Total (i.e. Null);  118 Residual\nNull Deviance:      94.24 \nResidual Deviance: 92.43    AIC: 96.43\n\n\n\nCalculating ROC-AUC for ‘DOSE’ model.\n\n# Accuracy and ROC-AUC for logistic model with DOSE\nacc_dose &lt;- logistic_dose %&gt;% \n  predict(new_data = selected_data, type = \"prob\") %&gt;% \n  bind_cols(selected_data) %&gt;% \n  roc_auc(truth = SEX, .pred_1) \n\nprint(acc_dose)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.592\n\n\n\nFitting model with ‘DOSE’ as the predictor.\n\n# Model with all predictors\n# Fit the model\nlogistic_all &lt;- logistic_spec %&gt;% \n  fit(SEX ~ ., data = selected_data)\n\nprint(logistic_all)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)            Y         DOSE          AGE        RACE2        RACE7  \n   60.32525     -0.00104     -0.03076      0.08340     -1.92738      0.11763  \n     RACE88           WT           HT  \n   -1.50012     -0.06283    -33.19601  \n\nDegrees of Freedom: 119 Total (i.e. Null);  111 Residual\nNull Deviance:      94.24 \nResidual Deviance: 32.08    AIC: 50.08\n\n\n\nCalculating ROC-AUC for all predictors model.\n\n# Accuracy and ROC-AUC for logistic model with all predictors\nacc_all &lt;- logistic_all %&gt;% \n  predict(new_data = selected_data, type = \"prob\") %&gt;% \n  bind_cols(selected_data) %&gt;% \n  roc_auc(truth = SEX, .pred_1)\n\nprint(acc_all)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.980\n\n\n\nBoth models show a significant amount of error (RMSE) and a moderate amount of variance (R-squared).\nHowever, including all predictors marginally improves both the RMSE and R-squared values, indicating a better predictive fit.\n\n\n\n\n\n\n\nLoading in kknn for nearest neighbor.\n\n# Load kknn for nearest neighbor\nlibrary(kknn)\n\n\nK-Nearest neighbors model for continuous outcome ‘Y’.\n\n# K-Nearest Neighbors Model for Continuous Outcome (Y)\n# Define KNN model specification for regression\nknn_spec_regression &lt;- nearest_neighbor(neighbors = 5) %&gt;%  # You can adjust the number of neighbors\n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"regression\")\n\n# Fit KNN model for Y with DOSE as the predictor\nknn_fit_Y_DOSE &lt;- knn_spec_regression %&gt;% \n  fit(Y ~ DOSE, data = selected_data)\n\n# Fit KNN model for Y with all predictors\nknn_fit_Y_all &lt;- knn_spec_regression %&gt;% \n  fit(Y ~ ., data = selected_data)\n\n# Assuming you have a test dataset or you split your selected_data into training and testing\nset.seed(123)  # For reproducibility\ndata_split &lt;- initial_split(selected_data, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n# Predictions\npredictions_Y_DOSE &lt;- predict(knn_fit_Y_DOSE, new_data = test_data) %&gt;% \n  bind_cols(test_data)\n\npredictions_Y_all &lt;- predict(knn_fit_Y_all, new_data = test_data) %&gt;% \n  bind_cols(test_data)\n\n# Compute RMSE and R-squared for both models\nmetrics_Y_DOSE &lt;- metrics(predictions_Y_DOSE, truth = Y, estimate = .pred)\nmetrics_Y_all &lt;- metrics(predictions_Y_all, truth = Y, estimate = .pred)\n\ncat(\"Y DOSE\\n\")\nprint(metrics_Y_DOSE)\ncat(\"\\n\")\ncat(\"Y All Predictors\\n\")\nprint(metrics_Y_all)\n\nY DOSE\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     576.   \n2 rsq     standard       0.562\n3 mae     standard     458.   \n\nY All Predictors\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     395.   \n2 rsq     standard       0.773\n3 mae     standard     329.   \n\n\n\nK-Nearest neighbors model for categorical outcome SEX.\n\n# K-Nearest Neighbors Model for Categorical Outcome (SEX)\n# Define KNN model specification for classification\nknn_spec_classification &lt;- nearest_neighbor(neighbors = 5) %&gt;%  # Adjust neighbors as needed\n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\")\n\n# Fit KNN model for SEX with DOSE as the predictor\nknn_fit_SEX_DOSE &lt;- knn_spec_classification %&gt;% \n  fit(SEX ~ DOSE, data = selected_data)\n\n# Fit KNN model for SEX with all predictors\nknn_fit_SEX_all &lt;- knn_spec_classification %&gt;% \n  fit(SEX ~ ., data = selected_data)\n\n# Predictions\npredictions_SEX_DOSE &lt;- predict(knn_fit_SEX_DOSE, new_data = test_data, type = \"prob\") %&gt;% \n  bind_cols(test_data)\n\npredictions_SEX_all &lt;- predict(knn_fit_SEX_all, new_data = test_data, type = \"prob\") %&gt;% \n  bind_cols(test_data)\n\n# Compute Accuracy and ROC-AUC for both models\nmetrics_SEX_DOSE &lt;- roc_auc(predictions_SEX_DOSE, truth = SEX, .pred_1) # Adjust based on factor levels\nmetrics_SEX_all &lt;- roc_auc(predictions_SEX_all, truth = SEX, .pred_1)\n\ncat(\"SEX DOSE\\n\")\nprint(metrics_SEX_DOSE)\ncat(\"\\n\")\ncat(\"SEX All Predictors\\n\")\nprint(metrics_SEX_all)\n\nSEX DOSE\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.409\n\nSEX All Predictors\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary             1"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#exercise-8",
    "href": "fitting-exercise/fitting-exercise.html#exercise-8",
    "title": "Fitting Exercise",
    "section": "",
    "text": "The first step was to import and read the data from the csv file.\nFirst, the required libraries ‘here’ and ‘readr’ were loaded.\n\n# Load here and readr for data reading\nlibrary(here)\nlibrary(readr)\n\n\n\nNext, the data was read from the csv file.\n\n# Use the here function to specify the file path\ndata_path &lt;- here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\")\n\n# Load the data\ndata &lt;- read_csv(data_path)\n\n\nPrinting the first few rows of the dataset, to make sure it loaded correctly.\n\n# Inspect the first few rows of the data to ensure it's loaded correctly\nhead(data)\n\n# A tibble: 6 × 17\n     ID   CMT  EVID  EVI2   MDV    DV  LNDV   AMT  TIME  DOSE   OCC  RATE   AGE\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   793     1     1     1     1     0  0       25 0        25     1    75    42\n2   793     2     0     0     0   491  6.20     0 0.2      25     1     0    42\n3   793     2     0     0     0   605  6.40     0 0.25     25     1     0    42\n4   793     2     0     0     0   556  6.32     0 0.367    25     1     0    42\n5   793     2     0     0     0   310  5.74     0 0.533    25     1     0    42\n6   793     2     0     0     0   237  5.47     0 0.7      25     1     0    42\n# ℹ 4 more variables: SEX &lt;dbl&gt;, RACE &lt;dbl&gt;, WT &lt;dbl&gt;, HT &lt;dbl&gt;\n\n\n\nLoading in ggplot2 for plotting.\nPlotting ‘DV’ as a function of time, stratified by ‘DOSE’ with ‘ID’ as a grouping factor. This provides a quick visual overview of the data.\n\n# Load ggplot2 for plotting\nlibrary(ggplot2)\n\n# Define colors\ncolors &lt;- c(\"#5c88da\", \"#84bd00\", \"#ffcd00\")\n\nggplot(data, aes(x = TIME, y = DV, group = ID, color = as.factor(DOSE))) +\n  geom_line() +\n  scale_color_manual(values = colors) +  \n  facet_wrap(~ DOSE, scales = \"free\", ncol = 1) +  \n  labs(title = \"DV vs. Time Stratified by DOSE\", x = \"Time\", y = \"DV\", color = \"DOSE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLoading in dplyr for data manipulation.\n\n# Load dplyr for data malipulation\nlibrary(dplyr)\n\n\nFirst, data is filtered to keep only rows where ‘OOC’ is equal to 1.\nThen the first few rows of the dataframe are printed, to ensure that the data was filtered correctly.\n\n# Filter the DataFrame to keep only rows where OCC equals 1\ndata1 &lt;- subset(data, OCC == 1)\n\n# Print the first few lines of the filtered dataframe\nprint(head(data1))\n\n# A tibble: 6 × 17\n     ID   CMT  EVID  EVI2   MDV    DV  LNDV   AMT  TIME  DOSE   OCC  RATE   AGE\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   793     1     1     1     1     0  0       25 0        25     1    75    42\n2   793     2     0     0     0   491  6.20     0 0.2      25     1     0    42\n3   793     2     0     0     0   605  6.40     0 0.25     25     1     0    42\n4   793     2     0     0     0   556  6.32     0 0.367    25     1     0    42\n5   793     2     0     0     0   310  5.74     0 0.533    25     1     0    42\n6   793     2     0     0     0   237  5.47     0 0.7      25     1     0    42\n# ℹ 4 more variables: SEX &lt;dbl&gt;, RACE &lt;dbl&gt;, WT &lt;dbl&gt;, HT &lt;dbl&gt;\n\n\n\nNext, any observations where ‘TIME’ equals 0 are filtered out.\nThen the sum of the DV variables are computed and assigned to variable Y.\nThen a dataframe is created, which only contains observations where ‘TIME’ equals 0.\nFinally, ‘inner_join’ is used to combine the dataframes.\n\n# Filter out observations where TIME is not equal to 0\ndata_filtered &lt;- filter(data1, TIME != 0)\n\n# Compute the sum of the DV variable for each individual\nY &lt;- data_filtered %&gt;%\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV))\n\n# Create a dataframe containing only the observations where TIME equals 0\ndata_TIME_0 &lt;- filter(data1, TIME == 0)\n\n# Combine the two dataframes using the appropriate join function\ncombined_data &lt;- inner_join(Y, data_TIME_0, by = \"ID\")\n\n\nNext, ‘RACE’ and ‘SEX’ are converted to factor variables.\nThen variables ‘Y’, ‘DOSE’, ‘AGE’, ‘SEX’, ‘RACE’, ‘WT’, and ‘HT’ are assigned to selected_data.\n\n# Convert RACE and SEX to factor variables\ncombined_data &lt;- combined_data %&gt;%\n  mutate(RACE = factor(RACE),\n         SEX = factor(SEX))\n\n# Select only the desired variables\nselected_data &lt;- combined_data %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT)\n\n\nSaving ‘selected_data’ to RDs file for use in module 11 exercise\n\n# Save cleaned data to RDs file for use in Exercise 11\nRDSfilepath &lt;- here(\"ml-models-exercise\", \"cleandata.rds\")\nwrite_rds(selected_data, RDSfilepath)\n\n\n\n\n\n\n\nLoading in packages for data exploration and visualization.\n\n# Load tidyr, knitr, ggsci, and corrplot for data exploration and visualization\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggsci)\nlibrary(corrplot)\n\n\nTo get a better idea of the variables, summary table is printed.\n\n# Summary table for all variables\nsummary_data &lt;- summary(selected_data[, c(\"Y\", \"DOSE\", \"AGE\", \"SEX\", \"RACE\", \"WT\", \"HT\")])\nprint(summary_data)\n\n       Y               DOSE            AGE        SEX     RACE   \n Min.   : 826.4   Min.   :25.00   Min.   :18.00   1:104   1 :74  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:26.00   2: 16   2 :36  \n Median :2349.1   Median :37.50   Median :31.00           7 : 2  \n Mean   :2445.4   Mean   :36.46   Mean   :33.00           88: 8  \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:40.25                  \n Max.   :5606.6   Max.   :50.00   Max.   :50.00                  \n       WT               HT       \n Min.   : 56.60   Min.   :1.520  \n 1st Qu.: 73.17   1st Qu.:1.700  \n Median : 82.10   Median :1.770  \n Mean   : 82.55   Mean   :1.759  \n 3rd Qu.: 90.10   3rd Qu.:1.813  \n Max.   :115.30   Max.   :1.930  \n\n\n\nCreating a scatterplot for ‘Y’ vs ‘DOSE’.\n\n# Define colors\ncolors &lt;- c(\"#cc1c00\", \"#5c88da\", \"#84bd00\", \"#ffcd00\")\n\n# Create the scatterplot of Y vs. DOSE \nggplot(selected_data, aes(x = DOSE, y = Y)) +\n  geom_point(aes(color = DOSE), alpha = 0.6, size = 3) +\n  scale_color_gradientn(colors = colors) +\n  labs(title = \"Y vs. DOSE\", x = \"Dose\", y = \"Y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 20),\n        axis.title.x = element_text(face=\"bold\", size = 14),\n        axis.title.y = element_text(face=\"bold\", size = 14),\n        legend.title = element_blank())\n\n\n\n\n\nCreating a scatterplot for ‘Y’ vs ‘AGE’.\n\n# Scatterplot of Y vs. AGE\nggplot(selected_data, aes(x = AGE, y = Y)) +\n  geom_point(aes(color = AGE), alpha = 0.6, size = 3) +\n  scale_color_gradient(low = \"#5c88da\", high = \"#ffcd00\") + \n  labs(title = \"Y vs. AGE\", x = \"Age\", y = \"Y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 20),\n        axis.title.x = element_text(face=\"bold\", size = 14),\n        axis.title.y = element_text(face=\"bold\", size = 14),\n        legend.title = element_text(size = 14))\n\n\n\n\n\nCreating a boxplot for ‘Y’ vs ‘SEX’.\n\n# Boxplot of Y vs. SEX \nggplot(selected_data, aes(x = as.factor(SEX), y = Y, fill = as.factor(SEX))) +\n  geom_boxplot() +\n  scale_fill_startrek() + \n  labs(title = \"Y vs. SEX\", x = \"Sex\", y = \"Y\") +\n  theme_minimal() +\n  theme(legend.title = element_blank(), \n        plot.title = element_text(hjust = 0.5),\n        axis.title.x = element_text(face=\"bold\"),\n        axis.title.y = element_text(face=\"bold\"))\n\n\n\n\n\nCreating a boxplot for ‘Y’ vs ‘RACE’.\n\n# Boxplot of Y vs. RACE\nggplot(selected_data, aes(x = as.factor(RACE), y = Y, fill = as.factor(RACE))) +\n  geom_boxplot() +\n  scale_fill_startrek() + \n  labs(title = \"Y vs. RACE\", x = \"Race\", y = \"Y\") +\n  theme_minimal() +\n  theme(legend.title = element_blank(), \n        plot.title = element_text(hjust = 0.5),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n        axis.title.x = element_text(face=\"bold\"),\n        axis.title.y = element_text(face=\"bold\"))\n\n\n\n\n\nCreating distribution curves for ‘WT’ and ‘HT’.\n\n# Distribution of WT with density curve\nggplot(selected_data, aes(x = WT)) + \n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = \"#5c88da\", color = \"black\") + \n  geom_density(alpha = 0.5, color = \"black\") + \n  theme_minimal() + \n  ggtitle(\"Distribution of WT\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n# Distribution of HT with density curve\nggplot(data, aes(x = HT)) + \n  geom_histogram(aes(y = ..density..), fill = \"#ffcd00\", color = \"black\") + \n  geom_density(alpha = 0.5, color = \"black\") + \n  theme_minimal() + \n  ggtitle(\"Distribution of HT\")\n\n\n\n\n\nPlotting correlation matrix.\n\n# Convert variables_of_interest to numeric\nvariables_of_interest &lt;- as.data.frame(sapply(selected_data, as.numeric))\n\n# Calculate correlation matrix for the variables of interest\ncorr_matrix &lt;- cor(variables_of_interest, use = \"complete.obs\")\n\n# Your custom colors\nmy_colors &lt;- colorRampPalette(c(\"#5c88da\", \"white\", \"#ffcd00\"))(200) \n\n# Plotting the correlation matrix with custom colors\ncorrplot(corr_matrix, method = \"color\", col = my_colors,\n         tl.col=\"black\", tl.srt=45) # Text label color and rotation\n\n\n\n\n\n\n\n\n\n\nLoading in tidymodels for model fitting.\n\n# Load tidymodels for model fitting \nlibrary(tidymodels)\n\n\n\n\n\nModels were fitted to the continuous outcome ‘Y’.\n\nDefining the linear model specification.\n\n# Define the model specification\nlinear_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n\nFitting model with ‘DOSE’ as the predictor.\n\n# Model with DOSE as predictor\n# Fit the model\nmodel_dose &lt;- linear_spec %&gt;% \n  fit(Y ~ DOSE, data = selected_data)\n\n# Summarize the model\nsummary(model_dose$fit)\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1284.91  -441.14   -97.22   325.84  2372.87 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  323.062    199.049   1.623    0.107    \nDOSE          58.213      5.194  11.208   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 672.1 on 118 degrees of freedom\nMultiple R-squared:  0.5156,    Adjusted R-squared:  0.5115 \nF-statistic: 125.6 on 1 and 118 DF,  p-value: &lt; 2.2e-16\n\n\n\nCalculating RMSE and R-squared for ‘DOSE’ model.\n\n# RMSE and R-squared for model with DOSE\nrmse_dose &lt;- model_dose %&gt;% \n  predict(new_data = selected_data) %&gt;% \n  bind_cols(selected_data) %&gt;% \n  metrics(truth = Y, estimate = .pred) %&gt;% \n  filter(.metric %in% c(\"rmse\", \"rsq\"))\n\nprint(rmse_dose)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n\n\n\nFitting model with all predictors.\n\n# Model with all predictors\n# Fit the model\nmodel_all &lt;- linear_spec %&gt;% \n  fit(Y ~ ., data = selected_data)\n\n# Summarize the model\nsummary(model_all$fit)\n\n\nCall:\nstats::lm(formula = Y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1480.86  -367.81   -79.95   266.15  2431.52 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3386.863   1835.153   1.846 0.067623 .  \nDOSE          59.935      4.880  12.281  &lt; 2e-16 ***\nAGE            3.155      7.825   0.403 0.687530    \nSEX2        -357.734    216.928  -1.649 0.101957    \nRACE2        155.034    128.627   1.205 0.230650    \nRACE7       -405.320    448.189  -0.904 0.367768    \nRACE88       -53.505    244.668  -0.219 0.827296    \nWT           -23.047      6.395  -3.604 0.000471 ***\nHT          -748.487   1103.979  -0.678 0.499188    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 614.3 on 111 degrees of freedom\nMultiple R-squared:  0.6193,    Adjusted R-squared:  0.5919 \nF-statistic: 22.57 on 8 and 111 DF,  p-value: &lt; 2.2e-16\n\n\n\nCalculating RMSE and R-squared for all predictors model.\n\n# RMSE and R-squared for model with all predictors\nrmse_all &lt;- model_all %&gt;% \n  predict(new_data = selected_data) %&gt;% \n  bind_cols(selected_data) %&gt;% \n  metrics(truth = Y, estimate = .pred) %&gt;% \n  filter(.metric %in% c(\"rmse\", \"rsq\"))\n\nprint(rmse_all)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     591.   \n2 rsq     standard       0.619\n\n\n\nThe lower RMSE and MAE values for the all predictors model compared to the ‘DOSE’ model indicate better accuracy in predicting Y when all predictors are considered.\nThe higher R-squared value for the all predictors model suggests that it does a better job in explaining variance in the data.\n\n\n\n\n\nModels were fitted to the categorical/binary outcome ‘SEX’.\nDefining the logistic model specification.\n\n# Define the model specification\nlogistic_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\")\n\n\nFitting model with ‘DOSE’ as the predictor.\n\n# Fit the model\nlogistic_dose &lt;- logistic_spec %&gt;% \n  fit(SEX ~ DOSE, data = selected_data)\n\nprint(logistic_dose)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ DOSE, family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n   -0.76482     -0.03175  \n\nDegrees of Freedom: 119 Total (i.e. Null);  118 Residual\nNull Deviance:      94.24 \nResidual Deviance: 92.43    AIC: 96.43\n\n\n\nCalculating ROC-AUC for ‘DOSE’ model.\n\n# Accuracy and ROC-AUC for logistic model with DOSE\nacc_dose &lt;- logistic_dose %&gt;% \n  predict(new_data = selected_data, type = \"prob\") %&gt;% \n  bind_cols(selected_data) %&gt;% \n  roc_auc(truth = SEX, .pred_1) \n\nprint(acc_dose)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.592\n\n\n\nFitting model with ‘DOSE’ as the predictor.\n\n# Model with all predictors\n# Fit the model\nlogistic_all &lt;- logistic_spec %&gt;% \n  fit(SEX ~ ., data = selected_data)\n\nprint(logistic_all)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)            Y         DOSE          AGE        RACE2        RACE7  \n   60.32525     -0.00104     -0.03076      0.08340     -1.92738      0.11763  \n     RACE88           WT           HT  \n   -1.50012     -0.06283    -33.19601  \n\nDegrees of Freedom: 119 Total (i.e. Null);  111 Residual\nNull Deviance:      94.24 \nResidual Deviance: 32.08    AIC: 50.08\n\n\n\nCalculating ROC-AUC for all predictors model.\n\n# Accuracy and ROC-AUC for logistic model with all predictors\nacc_all &lt;- logistic_all %&gt;% \n  predict(new_data = selected_data, type = \"prob\") %&gt;% \n  bind_cols(selected_data) %&gt;% \n  roc_auc(truth = SEX, .pred_1)\n\nprint(acc_all)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.980\n\n\n\nBoth models show a significant amount of error (RMSE) and a moderate amount of variance (R-squared).\nHowever, including all predictors marginally improves both the RMSE and R-squared values, indicating a better predictive fit.\n\n\n\n\n\n\n\nLoading in kknn for nearest neighbor.\n\n# Load kknn for nearest neighbor\nlibrary(kknn)\n\n\nK-Nearest neighbors model for continuous outcome ‘Y’.\n\n# K-Nearest Neighbors Model for Continuous Outcome (Y)\n# Define KNN model specification for regression\nknn_spec_regression &lt;- nearest_neighbor(neighbors = 5) %&gt;%  # You can adjust the number of neighbors\n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"regression\")\n\n# Fit KNN model for Y with DOSE as the predictor\nknn_fit_Y_DOSE &lt;- knn_spec_regression %&gt;% \n  fit(Y ~ DOSE, data = selected_data)\n\n# Fit KNN model for Y with all predictors\nknn_fit_Y_all &lt;- knn_spec_regression %&gt;% \n  fit(Y ~ ., data = selected_data)\n\n# Assuming you have a test dataset or you split your selected_data into training and testing\nset.seed(123)  # For reproducibility\ndata_split &lt;- initial_split(selected_data, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n# Predictions\npredictions_Y_DOSE &lt;- predict(knn_fit_Y_DOSE, new_data = test_data) %&gt;% \n  bind_cols(test_data)\n\npredictions_Y_all &lt;- predict(knn_fit_Y_all, new_data = test_data) %&gt;% \n  bind_cols(test_data)\n\n# Compute RMSE and R-squared for both models\nmetrics_Y_DOSE &lt;- metrics(predictions_Y_DOSE, truth = Y, estimate = .pred)\nmetrics_Y_all &lt;- metrics(predictions_Y_all, truth = Y, estimate = .pred)\n\ncat(\"Y DOSE\\n\")\nprint(metrics_Y_DOSE)\ncat(\"\\n\")\ncat(\"Y All Predictors\\n\")\nprint(metrics_Y_all)\n\nY DOSE\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     576.   \n2 rsq     standard       0.562\n3 mae     standard     458.   \n\nY All Predictors\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     395.   \n2 rsq     standard       0.773\n3 mae     standard     329.   \n\n\n\nK-Nearest neighbors model for categorical outcome SEX.\n\n# K-Nearest Neighbors Model for Categorical Outcome (SEX)\n# Define KNN model specification for classification\nknn_spec_classification &lt;- nearest_neighbor(neighbors = 5) %&gt;%  # Adjust neighbors as needed\n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\")\n\n# Fit KNN model for SEX with DOSE as the predictor\nknn_fit_SEX_DOSE &lt;- knn_spec_classification %&gt;% \n  fit(SEX ~ DOSE, data = selected_data)\n\n# Fit KNN model for SEX with all predictors\nknn_fit_SEX_all &lt;- knn_spec_classification %&gt;% \n  fit(SEX ~ ., data = selected_data)\n\n# Predictions\npredictions_SEX_DOSE &lt;- predict(knn_fit_SEX_DOSE, new_data = test_data, type = \"prob\") %&gt;% \n  bind_cols(test_data)\n\npredictions_SEX_all &lt;- predict(knn_fit_SEX_all, new_data = test_data, type = \"prob\") %&gt;% \n  bind_cols(test_data)\n\n# Compute Accuracy and ROC-AUC for both models\nmetrics_SEX_DOSE &lt;- roc_auc(predictions_SEX_DOSE, truth = SEX, .pred_1) # Adjust based on factor levels\nmetrics_SEX_all &lt;- roc_auc(predictions_SEX_all, truth = SEX, .pred_1)\n\ncat(\"SEX DOSE\\n\")\nprint(metrics_SEX_DOSE)\ncat(\"\\n\")\ncat(\"SEX All Predictors\\n\")\nprint(metrics_SEX_all)\n\nSEX DOSE\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.409\n\nSEX All Predictors\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary             1"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#exercise-10",
    "href": "fitting-exercise/fitting-exercise.html#exercise-10",
    "title": "Fitting Exercise",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nSetup\nLoad nessecary libraries and assign random seed value.\n\n# Load required libraries\nlibrary(dplyr)\n\n# Assign seed value to `rngseed`\nrngseed = 1234\n\n\nRemove variable RACE from dataset, and set random seed.\n\n# Remove RACE variable from data\ndata_pt10 &lt;- select(selected_data, -RACE)\n\n# Set random seed\nset.seed(rngseed)\n\n\n\n\nData Splitting\nSplit data into training data and test data. Data is split at a 3 to 4 ratio, with 75 training data and .25 test data.\n\n# Splitting the data\ndata_split &lt;- initial_split(data_pt10, prop = 3/4)\n\n# Create data frames for the training and test sets\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\n\n\n\n\nModel Fitting\nLoad required libraries and set seed for reproducability.\n\n# Load required libraries\nlibrary(tidymodels)\nlibrary(dplyr)\n\n\nFit linear models with Y as the outcome.\nlinfit1 uses DOSE as the predictor. linfit2 uses all predictors.\n\n# Fit linear models with Y as outcome\nlin_mod &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nlinfit1 &lt;- lin_mod %&gt;% fit(Y ~ DOSE, data = train_data) # only DOSE as predictor\nlinfit2 &lt;- lin_mod %&gt;% fit(Y ~ ., data = train_data) # all predictors\n\n\nComputing RMSE and R-squared scores for both models.\n\n# Compute the RMSE and R squared for DOSE\nmetrics_1 &lt;- linfit1 %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\n# Compute the RMSE and R squared for all predictors model \nmetrics_2 &lt;- linfit2 %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\n# Print the results\ncat(\"DOSE Model\\n\")\nprint(metrics_1)\ncat(\"\\n\")\ncat(\"All Predictors Model\\n\")\nprint(metrics_2)\n\nDOSE Model\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     703.   \n2 rsq     standard       0.451\n3 mae     standard     546.   \n\nAll Predictors Model\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     627.   \n2 rsq     standard       0.562\n3 mae     standard     486.   \n\n\n\nFit null model.\n\n# Fit null model\nnullfit &lt;- lin_mod %&gt;% fit(Y ~ 1, data = train_data)\n\n\nCompute RMSE and R-squared scores for null model.\n\n# Compute the RMSE and R squared for the null model\nmetrics_null &lt;- nullfit %&gt;%\n  predict(train_data) %&gt;%\n  bind_cols(train_data) %&gt;%\n  metrics(truth = Y, estimate = .pred)\n\n# Print the results for the null model\ncat(\"Null Model\\n\")\nprint(metrics_null)\n\nNull Model\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        948.\n2 rsq     standard         NA \n3 mae     standard        765.\n\n\n\n\n\n\nResampling Model Assessment\nLoad required libraries, and set seed for reproducability.\n\n# Load required libraries\nlibrary(tidymodels)\nlibrary(dplyr)\n\n# Set the seed for reproducibility\nset.seed(rngseed) \n\n\nCreate CV folds. We are using a 10-fold cross-validation.\n\n# Create CV Folds \ncv_folds &lt;- vfold_cv(train_data, v = 10, strata = NULL) \n\n\nCreating workflows for DOSE and all predictors models.\n\n# Creating workflows\n\n# DOSE model\nworkflow_dose &lt;- workflow() %&gt;%\n  add_model(lin_mod) %&gt;%\n  add_formula(Y ~ DOSE)\n\n# All predictors model\nworkflow_all &lt;- workflow() %&gt;%\n  add_model(lin_mod) %&gt;%\n  add_formula(Y ~ .)\n\n\nFitting the models with resampling, and calculating RMSE and R-squared values.\n\n# DOSE model\ncv_results_dose &lt;- fit_resamples(\n  workflow_dose,\n  cv_folds,\n  metrics = metric_set(rmse, rsq)\n)\n\n# All predictors model\ncv_results_all &lt;- fit_resamples(\n  workflow_all,\n  cv_folds,\n  metrics = metric_set(rmse, rsq)\n)\n\n\nSummarizing model results.\n\n# Summarize model results\ncat(\"DOSE Model with Resampling\\n\")\ncollect_metrics(cv_results_dose)\ncat(\"\\n\")\ncat(\"All Predictors Model with Resampling\\n\")\ncollect_metrics(cv_results_all)\n\nDOSE Model with Resampling\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   691.       10 67.5    Preprocessor1_Model1\n2 rsq     standard     0.512    10  0.0592 Preprocessor1_Model1\n\nAll Predictors Model with Resampling\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   646.       10 64.8    Preprocessor1_Model1\n2 rsq     standard     0.573    10  0.0686 Preprocessor1_Model1\n\n\n\n\n\n\nComparing Results\n\nDose Models\nAs can be seen in the charts below, the model with resampling is a slightly better fit than the model without. The RMSE of the model with resampling (691) is lower than the RMSE of the model without resampling (703), indicating a better fit. The R-squared of the model with resampling (0.512) is slightly higher than the model without resampling (.451), indicating a better fit.\n\ncat(\"DOSE Model with Resampling\\n\")\ncollect_metrics(cv_results_dose)\ncat(\"\\n\")\ncat(\"DOSE Model\\n\")\nprint(metrics_1)\n\nDOSE Model with Resampling\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   691.       10 67.5    Preprocessor1_Model1\n2 rsq     standard     0.512    10  0.0592 Preprocessor1_Model1\n\nDOSE Model\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     703.   \n2 rsq     standard       0.451\n3 mae     standard     546.   \n\n\n\n\n\n\nAll Predictors Models\nAs can be seen in the charts below, the model with resampling is a slightly better fit than the model without, but not as good an improvement as with the DOSE models. The RMSE of the model with resampling (646) is actually higher than the RMSE of the model without resampling (627), suggesting a slight increase in prediction errors with the resampling model. The R-squared of the model with resampling (0.573) is slightly higher than the model without resampling (0.562), indicating a better fit. Though this increase is not as notable as the one seen with the DOSE models.\n\ncat(\"All Predictors Model with Resampling\\n\")\ncollect_metrics(cv_results_all)\ncat(\"\\n\")\ncat(\"All Predictors Model\\n\")\nprint(metrics_2)\n\nAll Predictors Model with Resampling\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   646.       10 64.8    Preprocessor1_Model1\n2 rsq     standard     0.573    10  0.0686 Preprocessor1_Model1\n\nAll Predictors Model\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     627.   \n2 rsq     standard       0.562\n3 mae     standard     486.   \n\n\n\n\n\n\n\nUsing a New Seed\nThis follows the same steps as above, but with a new random seed of 042.\nSetting seed for reproducability, and creating CV folds.\n\n# Set the seed for reproducibility\nset.seed(42) \n\n# Create CV Folds \ncv_folds &lt;- vfold_cv(train_data, v = 10, strata = NULL) \n\n\nCreating workflows for DOSE and all predictors models.\n\n# DOSE model\nworkflow_dose &lt;- workflow() %&gt;%\n  add_model(lin_mod) %&gt;%\n  add_formula(Y ~ DOSE)\n\n# All predictors model\nworkflow_all &lt;- workflow() %&gt;%\n  add_model(lin_mod) %&gt;%\n  add_formula(Y ~ .)\n\n\nFitting the models with resampling, and calculating RMSE and R-squared values.\n\n# DOSE model\ncv_results_dose &lt;- fit_resamples(\n  workflow_dose,\n  cv_folds,\n  metrics = metric_set(rmse, rsq)\n)\n\n# All predictors model\ncv_results_all &lt;- fit_resamples(\n  workflow_all,\n  cv_folds,\n  metrics = metric_set(rmse, rsq)\n)\n\n\nSummarizing model results.\n\n# Summarize model results\ncat(\"(New Seed) DOSE Model with Resampling\\n\")\ncollect_metrics(cv_results_dose)\ncat(\"(New Seed) All Predictors Model with Resampling\\n\")\ncollect_metrics(cv_results_all)\n\n(New Seed) DOSE Model with Resampling\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   696.       10 63.8    Preprocessor1_Model1\n2 rsq     standard     0.460    10  0.0853 Preprocessor1_Model1\n(New Seed) All Predictors Model with Resampling\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   642.       10 55.8    Preprocessor1_Model1\n2 rsq     standard     0.507    10  0.0845 Preprocessor1_Model1\n\n\n\n\n\n\nComparing Results (Again)\n\nDose Models\nAs can be seen in the charts below, the model with resampling is a slightly better fit than the model without. The RMSE of the model with resampling (696) is lower than the RMSE of the model without resampling (703), indicating a better fit. The R-squared of the model with resampling (0.461) is slightly higher than the model without resampling (.451), indicating a better fit.\n\ncat(\"(New Seed) DOSE Model with Resampling\\n\")\ncollect_metrics(cv_results_dose)\ncat(\"\\n\")\ncat(\"DOSE Predictors Model\\n\")\nprint(metrics_1)\n\n(New Seed) DOSE Model with Resampling\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   696.       10 63.8    Preprocessor1_Model1\n2 rsq     standard     0.460    10  0.0853 Preprocessor1_Model1\n\nDOSE Predictors Model\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     703.   \n2 rsq     standard       0.451\n3 mae     standard     546.   \n\n\n\n\n\n\nAll Predictors Models\nAs can be seen in the charts below, the model with resampling is not overall a much better fit than without resampling. The RMSE of the model with resampling (642) is actually higher than the RMSE of the model without resampling (627), suggesting a slight increase in prediction errors with the resampling model. The R-squared of the model with resampling (0.507) is slightly lower than the model without resampling (0.562), indicating a similar or worse fit.\n\ncat(\"All Predictors Model with Resampling\\n\")\ncollect_metrics(cv_results_all)\ncat(\"\\n\")\ncat(\"All Predictors Model\\n\")\nprint(metrics_2)\n\nAll Predictors Model with Resampling\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   642.       10 55.8    Preprocessor1_Model1\n2 rsq     standard     0.507    10  0.0845 Preprocessor1_Model1\n\nAll Predictors Model\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     627.   \n2 rsq     standard       0.562\n3 mae     standard     486.   \n\n\n\n\n\n\nThoughts\nBoth runs of the resampling followed a similar pattern when run even with different random seeds. The DOSE models showed a better fit with resampling, while the all predictors models showed little to no indication of a better fit.\n\n\n\n\n\nThis section added by Ranni Tewfik.\nFirst let’s create a figure that plots observed values versus predicted values for the three original model fits to all of the training data.\n\n#Compute the predicted values for the three models\npredicted1 &lt;- predict(linfit1, new_data = train_data)\npredicted2 &lt;- predict(linfit2, new_data = train_data)\npredictednull &lt;- predict(nullfit, new_data = train_data)\n\n#Create a data frame with the observed values and predicted values from the three models\npredictions &lt;- data.frame(\n  observed = train_data$Y,\n  model1 = predicted1,\n  model2 = predicted2,\n  null = predictednull)\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nggplot(predictions, aes(x = observed)) +\n  geom_point(aes(y = .pred, color = \"Model 1\"), alpha = 0.7) +\n  geom_point(aes(y = .pred.1, color = \"Model 2\"), alpha = 0.7) +\n  geom_point(aes(y = .pred.2, color = \"Null Model\"), alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Three Models\") +\n  theme_minimal()\n\n\n\n\nAs expected, the predicted values from the null model are a straight horizontal line because the same value (mean) is predicted for each observation. For Model 1 with only DOSE as the predictor, the data fall along three horizontal lines (i.e., only three different predicted values for the outcome) because the DOSE variable takes only three values. Model 2 with all predictors looks the best because the points fall relatively along the dashed diagonal line (i.e., observed and predicted values generally agree), although there is some scatter along the diagonal line. There seems to be a pattern to the scatter, as the predicted values are lower than the observed values for high values. Perhaps there are aspects of the outcome pattern that the model cannot explain.\nNow let’s create a figure that plots predicted values versus residuals for Model 2.\n\n#Fit the linear model for all predictors and create a data frame\nlinmodel2 &lt;- lm(Y ~ ., data = train_data)\nlinmodel2f &lt;- fortify(linmodel2)\n\n#Create a figure that plots predicted values on the x-axis and residuals on the y-axis\nggplot(linmodel2f, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-2500, 2500) +\n  labs(x = \"Predicted Values\", y = \"Residuals\", title = \"Predicted Values vs. Residuals for Model 2\") +\n  theme_minimal()\n\n\n\n\nThere is a discernible pattern in the plot of predicted values versus residuals, as there are more and higher negative values of residuals compared to positive values. It may be that more variables are needed (i.e., important information is missing) or that the model is too simple (i.e., the outcome depends on some variable in a nonlinear relationship).\nFinally, let’s use the bootstrap method to sample the data, fit Model 2 to the data, and measure uncertainty in our predictions. Let’s create a figure that plots observed versus predicted values for Model 2.\n\n#Load the required package\nlibrary(rsample)\nlibrary(boot)\n\n#Set a seed\nrngseed = 1234\nset.seed(rngseed)\n\n#Create 100 bootstraps using the training data\ndat_bs &lt;- bootstraps(train_data, times = 100)\n\n#Create an empty matrix to store predictions\npred_bs &lt;- matrix(nrow = nrow(train_data), ncol = 100)\n\n#Use a loop to fit Model 2 to each bootstrap sample and make predictions for the training data\nfor(i in 1:100) {\n  dat_sample &lt;- analysis(dat_bs$splits[[i]])\n  model &lt;- lm(Y ~ ., data = dat_sample)\n  predictions &lt;- predict(model, newdata = train_data)\n  pred_bs[, i] &lt;- predictions\n}\n\n#Compute the median and 89% confidence intervals\npreds &lt;- apply(pred_bs, 1, function(x) {quantile(x, c(0.055, 0.5, 0.945))}) %&gt;% t()\npreds &lt;- data.frame(preds)\n\n#Add the median and confidence intervals to the same data frame as the observed and predicted values for Model 2\nlinmodel2f$lower &lt;- preds$X5.5.\nlinmodel2f$median &lt;- preds$X50.\nlinmodel2f$upper &lt;- preds$X94.5.\n\n#Create a figure that plots observed values on the x-axis and predicted values on the y-axis\nggplot(linmodel2f, aes(x = Y)) +\n  geom_point(aes(y = .fitted, color = \"Point Estimates\"), alpha = 0.7) +\n  geom_point(aes(y = lower, color = \"Lower Bounds\"), alpha = 0.7) +\n  geom_point(aes(y = median, color = \"Medians\"), alpha = 0.7) +\n  geom_point(aes(y = upper, color = \"Upper Bounds\"), alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  xlim(0, 5000) +\n  ylim(0, 5000) +\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Model 2\") +\n  theme_minimal()\n\n\n\n\nIn general, the medians and confidence intervals in the plot follow the same diagonal pattern as the point estimates (i.e., observed and predicted values generally agree), although there is some scatter along the diagonal line. Because some predicted values are lower than the observed values for high values, there may be aspects of the outcome that the model cannot explain.\n\n\n\n\n\n\nFinal Evaluation\nGathering predictions for testing and training data.\n\n# Predictions for the training data\npredictions_train &lt;- predict(linfit2, new_data = train_data) %&gt;%\n  bind_cols(train_data) %&gt;%\n  mutate(dataset = \"Training\")\n\n# Predictions for the test data\npredictions_test &lt;- predict(linfit2, new_data = test_data) %&gt;%\n  bind_cols(test_data) %&gt;%\n  mutate(dataset = \"Test\")\n\n\nCombining predictions in to one data frame for plotting purposes.\n\n# Combine the predictions into one data frame\npredictions_combined &lt;- bind_rows(predictions_train, predictions_test)\n\n\nPlotting the graph using ggplot.\n\n# Creating the plot\nggplot(predictions_combined, aes(x = Y, y = .pred)) +\n  geom_point(aes(shape = dataset, color = dataset), alpha = 0.7, size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"grey40\", size = 1) +\n  scale_shape_manual(values = c(\"Training\" = 16, \"Test\" = 17)) + # Circle and triangle\n  scale_color_manual(values = c(\"Training\" = \"#ffcd00\", \"Test\" = \"#5c88da\")) +\n  labs(x = \"Observed Values\",\n       y = \"Predicted Values\",\n       title = \"Comparison of Observed vs. Predicted Values\",\n       subtitle = \"Includes both Training and Test Data\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\",\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16),\n        axis.title.x = element_text(size = 16, face = \"bold\"),\n        axis.title.y = element_text(size = 16, face = \"bold\"),\n        legend.text = element_text(size = 14)) +\n  guides(color = guide_legend(override.aes = list(size = 6)))\n\n\n\n\n\n\n\nOverall Model Assessment\nBoth model 1 and model 2 performed better than the null model, which was expected to be the case.\nThe DOSE model, while not much better than the null model, technically does improve results over the null model. I think the results of the DOSE model do make sense given there being only one predictor and thus limited observation of complexity within the data, however, I would not consider this model usable for any real purposes.\nModel 2 (all predictors model) does further improve results. The results here also make sense, as now there are many more predictors, which allow the model to capture more complexity and variance within the data. I would consider this model usable (depending on the application), though there is certainly room for improvement and further adjustment."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "ML-Models Exercise",
    "section": "",
    "text": "First, the required libraries were loaded.\n\n# Load required libraries\nlibrary(here)\nlibrary(readr)\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(dplyr)\nlibrary(ranger)\nlibrary(glmnet)\nlibrary(yardstick)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nLoad in cleaned data from previous exercise and set value for rngseed.\n\n# Load in cleaned data \ndata &lt;- readRDS(here(\"ml-models-exercise\", \"cleandata.rds\"))\n\n# Set value for rndseed\nrngseed &lt;- 1234\n\n\nProcessing the RACE variable so that values of “7” or “88” are now represented by the number “3”.\n\n# Replace 7 and 88 with 3 for RACE variable \nlevels(data$RACE)[levels(data$RACE) == \"7\" | levels(data$RACE) == \"88\"] &lt;- \"3\"\n\n\nDoing pairwise correlations.\n\n# Select only continuous variables\ndata_continuous &lt;- data[, c(\"DOSE\", \"AGE\", \"WT\", \"HT\")]\n\n# Create a pairwise correlation plot\npairwise_plot &lt;- ggpairs(data_continuous[, c(\"DOSE\", \"AGE\", \"WT\", \"HT\")],\n                         upper = list(continuous = wrap(\"cor\", size = 4)),\n                         lower = list(continuous = wrap(\"points\", size = 1, alpha = 0.5, colour = \"#5c88da\")),\n                         diag = list(continuous = wrap(\"barDiag\", fill = \"#ffcd00\"))\n) +\n  theme_light() +  \n  ggtitle(\"Pairwise Correlation Plot\") +  \n  theme(plot.title = element_text(hjust = 0.5),  \n        text = element_text(size = 12),  \n        axis.text.x = element_text(angle = 45, hjust = 1))  \n\nprint(pairwise_plot)\n\n\n\n\n\n\n\n\nCreating a dataframe for data with BMI, then calculating BMI based on WT and HT. Here WT is assumed to be measured in kilograms and HT is assumed to be measured in meters.\n\n# Creating dataframe data_wBMI\ndata_wBMI &lt;- data\n\n# Calculating BMI based on WT (kg) and HT (m) variables\ndata_wBMI$BMI &lt;- data$WT / (data$HT^2)\n\n\n\n\n\n\nWe are using three models in this exercise, a linear model, a LASSO regression model, and a random forest model.\nDefining the models.\n\n# Linear model with all predictors \nlinear_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# LASSO regression model\nlasso_model &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Random forest model\nrandom_forest_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", seed = rngseed) %&gt;%\n  set_mode(\"regression\")\n\n\nCreating workflows for the models.\n\n# Creating workflows\n# Define recipes\nrecipe &lt;- recipe(Y ~ ., data = data_wBMI)\nrecipe_lasso &lt;- recipe(Y ~ ., data = data_wBMI) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# Linear model workflow\nlinear_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(linear_model)\n\n# LASSO model workflow\nlasso_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_lasso) %&gt;%\n  add_model(lasso_model)\n\n# Random forest model workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(random_forest_model)\n\n\nFitting the models.\n\n# Fitting the models\n# Fit the linear model\nlinear_fit &lt;- fit(linear_workflow, data = data_wBMI)\n\n# Fit the LASSO model\nlasso_fit &lt;- fit(lasso_workflow, data = data_wBMI)\n\n# Fit the random forest model\nrf_fit &lt;- fit(rf_workflow, data = data_wBMI)\n\n\nMaking predictions for the models.\n\nlinear_preds &lt;- predict(linear_fit, new_data = data_wBMI) %&gt;%\n  bind_cols(data_wBMI) %&gt;%\n  rename(predicted = .pred)\n\n# Predictions for the LASSO model\nlasso_preds &lt;- predict(lasso_fit, new_data = data_wBMI) %&gt;%\n  bind_cols(data_wBMI) %&gt;%\n  rename(predicted = .pred)\n\n# Predictions for the random forest model\nrf_preds &lt;- predict(rf_fit, new_data = data_wBMI) %&gt;%\n  bind_cols(data_wBMI) %&gt;%\n  rename(predicted = .pred)\n\n\nCalculating the RMSE for each model.\n\n# Calculating RMSE for each model\nlinear_rmse &lt;- rmse(linear_preds, truth = Y, estimate = predicted) # Linear Model\nlasso_rmse &lt;- rmse(lasso_preds, truth = Y, estimate = predicted) # LASSO Model\nrf_rmse &lt;- rmse(rf_preds, truth = Y, estimate = predicted) # Random Forest Model\n\n\nVisualizing model fit by plotting predicted vs observed values for each model.\n\n# Plotting first fit\n# Define colors and shapes for the legend\ncolors &lt;- c(\"Predicted\" = \"#5c88da\", \"Observed\" = \"#ffcd00\", \"Reference Line\" = \"gray\")\n\n# Linear Model\np_linear &lt;- ggplot(data_wBMI, aes(x = Y)) +\n  geom_point(aes(y = linear_preds$predicted, color = \"Predicted\"), show.legend = FALSE) +\n  geom_point(aes(y = Y, color = \"Observed\"), show.legend = FALSE) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = colors[\"Reference Line\"]) +\n  labs(title = \"Linear Model\", y = \"Predicted\", x = \"Observed\") +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  theme(aspect.ratio = 1) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme(axis.title.x = element_text(size = 8), \n        axis.title.y = element_text(size = 8)) +\n  theme(text = element_text(family = 'mono')) +\n  theme(title = element_text(size = 10))\n\n\n# LASSO Model\np_lasso &lt;- ggplot(data_wBMI, aes(x = Y)) +\n  geom_point(aes(y = lasso_preds$predicted, color = \"Predicted\")) +\n  geom_point(aes(y = Y, color = \"Observed\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = colors[\"Reference Line\"]) +\n  labs(title = \"LASSO Model\", y = \"Predicted\", x = \"Observed\", color = \"Legend\") +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  theme(aspect.ratio = 1) +\n  theme(legend.text = element_text(size = 6)) +\n  theme(legend.title = element_text(size = 6)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme(axis.title.x = element_text(size = 8), \n        axis.title.y = element_text(size = 8)) +\n  theme(text = element_text(family = 'mono')) +\n  theme(title = element_text(size = 10))\n\n\n# Random Forest Model\np_rf &lt;- ggplot(data_wBMI, aes(x = Y)) +\n  geom_point(aes(y = rf_preds$predicted, color = \"Predicted\"), show.legend = FALSE) +\n  geom_point(aes(y = Y, color = \"Observed\"), show.legend = FALSE) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = colors[\"Reference Line\"]) +\n  labs(title = \"RF Model\", y = \"Predicted\", x = \"Observed\") +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  theme(aspect.ratio = 1)+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme(axis.title.x = element_text(size = 8), \n        axis.title.y = element_text(size = 8)) +\n  theme(text = element_text(family = 'mono')) +\n  theme(title = element_text(size = 10))\n\n# Combine plots\np_combined &lt;- (p_rf+ p_linear + p_lasso) + plot_layout(ncol = 3)\np_combined &lt;- p_combined + \n  plot_annotation(caption = 'First Fit - Predicted vs Observed Values') +\n  theme(plot.caption = element_text(size = 20), text = element_text(family = 'mono'))\n\nprint(p_combined)\n\n\n\n\n\nPrinting a table of RMSE values for each model.\n\n# Create a data frame with your RMSE values\nrmse_data &lt;- data.frame(\n  Model = c(\"Linear Regression\", \"Lasso Regression\", \"Random Forest\"),\n  RMSE = c(linear_rmse$.estimate, lasso_rmse$.estimate, rf_rmse$.estimate)\n)\n\n# Print the table using kable and kableExtra\nkable(rmse_data, \"html\", align = \"c\", caption = \"RMSE Values for Different Models\") %&gt;%\n  kable_styling(full_width = TRUE)\n\n\nRMSE Values for Different Models\n\n\nModel\nRMSE\n\n\n\n\nLinear Regression\n571.5954\n\n\nLasso Regression\n571.6504\n\n\nRandom Forest\n358.8240\n\n\n\n\n\n\n\n\n\nThe LASSO model and linear model perform similarly in this case. This could be due to the fact that in cases where there is little multicollinearity among predictors and the underlying model is not too complex, the LASSO model tends to perform in a similar way to the linear regression model.\n\n\n\n\n\n\n\n\n\nDefining penalty grid values and adjusting the model for tuning.\n\n# Define a grid of penalty values correctly\npenalty_grid &lt;- grid_regular(penalty(range = c(log10(1E-5), log10(1E2))), levels = 50)\n\n# Adjust the LASSO model specification for tuning\nlasso_model_tune &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n\nCreating workflow and resampling using the apparent function.\n\n# Create workflow\nlasso_workflow_tune &lt;- workflow() %&gt;%\n  add_recipe(recipe_lasso) %&gt;%  # Make sure recipe_lasso is defined correctly\n  add_model(lasso_model_tune)\n\n# Create an apparent resamples object for the apparent error rate\napparent_resamples &lt;- apparent(data_wBMI)\n\n\nTuning the model and visualizing with autoplot.\n\n# Tune the model\ntuning_results &lt;- tune_grid(\n  object = lasso_workflow_tune,\n  resamples = apparent_resamples,\n  grid = penalty_grid\n)\n\n\n# Visualize the tuning process\nlassobadplot &lt;- autoplot(tuning_results)\nlassobadplot + ggtitle(\"LASSO Model Tuned (Bad)\")\n\n\n\n\n\n\n\n\nLASSO does best (lowest RMSE) for low penalty values and gets worse as the penalty value increases. RMSE only increases and does not go lower than the value from the linear model. Why?\nAs the penalty parameter in LASSO increases, the model simplifies by reducing the number of features. It does this to prevent overfitting, but in some cases it can lead to underfitting. The lowest RMSE that LASSO can achieve will not be lower than the linear regression model because the penalty introduces bias which reduces variance.\n\n\n\n\n\n\nDefining model with tunable parameters and creating a tuning grid.\n\n# Define the random forest model with tunable parameters\nrandom_forest_model_tune &lt;- rand_forest(trees = 300, mtry = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\", seed = rngseed) %&gt;%\n  set_mode(\"regression\")\n\n# Create a tuning grid\ntuning_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7\n)\n\n\nCreating a workflow and tuning the random forest model, then visualizing with autoplot.\n\n# Create workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(random_forest_model_tune)\n\n# Tune random forest model\ntune_res &lt;- tune_grid(\n  rf_workflow,\n  resamples = vfold_cv(data_wBMI, v = 5), # Adjust the number of folds if necessary\n  grid = tuning_grid\n)\n\n\n# Visualizing with autoplot\nrfbadplot &lt;- autoplot(tune_res)\nrfbadplot + ggtitle(\"Random Forest Model Tuned (Bad)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefining penalty grid values and adjusting the model for tuning.\n\n# Define a grid of penalty values correctly\npenalty_grid &lt;- grid_regular(penalty(range = c(log10(1E-5), log10(1E2))), levels = 50)\n\n# Adjust the LASSO model specification for tuning\nlasso_model_tune_cv &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n\nAdjusting the workflow and creating 5-fold cross-validation resamples.\n\n# Adjust the workflow for the tunable model\nlasso_workflow_tune_cv &lt;- workflow() %&gt;%\n  add_recipe(recipe_lasso) %&gt;%  # Make sure recipe_lasso is defined correctly\n  add_model(lasso_model_tune_cv)\n\n# Create the cross-validation resamples\ncv_resamples &lt;- vfold_cv(data_wBMI, v = 5, repeats = 5)\n\n\nTuning the LASSO model using cross-validation and then visualizing with autoplot.\n\n# Tune LASSO model using cross-validation \ntuning_results_lasso_cv &lt;- tune_grid(\n  object = lasso_workflow_tune_cv,\n  resamples = cv_resamples,\n  grid = penalty_grid\n)\n\n# Plot the tuning results\nlassocvplot &lt;- autoplot(tuning_results_lasso_cv)\nlassocvplot + ggtitle(\"LASSO Model Tuned w/ CV\")\n\n\n\n\n\n\n\n\n\nDefining model with tunable parameters and creating a tuning grid.\n\n# Define the random forest model with tunable parameters\nrandom_forest_model_tune_cv &lt;- rand_forest(trees = 300, mtry = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\", seed = rngseed) %&gt;%\n  set_mode(\"regression\")\n\n# Create a tuning grid\ntuning_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7\n)\n\n\nCreating a workflow and tuning the random forest model using cross-validation, then visualizing with autoplot.\n\n# Create workflow\nrf_workflow_cv &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(random_forest_model_tune_cv)\n\n# Tune the random forest model\ntune_res_cv &lt;- tune_grid(\n  rf_workflow_cv,\n  resamples = cv_resamples,\n  grid = tuning_grid\n)\n\n# Plot the tuning results\nrfcvplot &lt;- autoplot(tune_res_cv)\nrfcvplot + ggtitle(\"Random Forest Model Tuned w/ CV\")\n\n\n\n\n\n\n\n\n\n\nThe LASSO models both follow a similar trend, the lower values are better and both perform worse with too much regularization. However, the “Bad” version seemed to initally be a better fit until the end where it began to overfit and then severely underperform.\n\n\n\nThe RF models also both follow a similar trend, as the number of randomly selected predictors increases, the RMSE decreases and stabalizes and the R-square value increases with the number of predictors. However, the “Bad” version has overall higher RMSE values and lower R-square values, which could indicate more overfitting.\n\n\n\n\n\n\nLASSO performs the best. Based on the autoplot graph, the LASSO model has a lower RMSE compared to the RF model’s best RMSE values. Similarly the LASSO model has smaller R-squared values than the RF model."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#exercise-11",
    "href": "ml-models-exercise/ml-models-exercise.html#exercise-11",
    "title": "ML-Models Exercise",
    "section": "",
    "text": "First, the required libraries were loaded.\n\n# Load required libraries\nlibrary(here)\nlibrary(readr)\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(dplyr)\nlibrary(ranger)\nlibrary(glmnet)\nlibrary(yardstick)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nLoad in cleaned data from previous exercise and set value for rngseed.\n\n# Load in cleaned data \ndata &lt;- readRDS(here(\"ml-models-exercise\", \"cleandata.rds\"))\n\n# Set value for rndseed\nrngseed &lt;- 1234\n\n\nProcessing the RACE variable so that values of “7” or “88” are now represented by the number “3”.\n\n# Replace 7 and 88 with 3 for RACE variable \nlevels(data$RACE)[levels(data$RACE) == \"7\" | levels(data$RACE) == \"88\"] &lt;- \"3\"\n\n\nDoing pairwise correlations.\n\n# Select only continuous variables\ndata_continuous &lt;- data[, c(\"DOSE\", \"AGE\", \"WT\", \"HT\")]\n\n# Create a pairwise correlation plot\npairwise_plot &lt;- ggpairs(data_continuous[, c(\"DOSE\", \"AGE\", \"WT\", \"HT\")],\n                         upper = list(continuous = wrap(\"cor\", size = 4)),\n                         lower = list(continuous = wrap(\"points\", size = 1, alpha = 0.5, colour = \"#5c88da\")),\n                         diag = list(continuous = wrap(\"barDiag\", fill = \"#ffcd00\"))\n) +\n  theme_light() +  \n  ggtitle(\"Pairwise Correlation Plot\") +  \n  theme(plot.title = element_text(hjust = 0.5),  \n        text = element_text(size = 12),  \n        axis.text.x = element_text(angle = 45, hjust = 1))  \n\nprint(pairwise_plot)\n\n\n\n\n\n\n\n\nCreating a dataframe for data with BMI, then calculating BMI based on WT and HT. Here WT is assumed to be measured in kilograms and HT is assumed to be measured in meters.\n\n# Creating dataframe data_wBMI\ndata_wBMI &lt;- data\n\n# Calculating BMI based on WT (kg) and HT (m) variables\ndata_wBMI$BMI &lt;- data$WT / (data$HT^2)\n\n\n\n\n\n\nWe are using three models in this exercise, a linear model, a LASSO regression model, and a random forest model.\nDefining the models.\n\n# Linear model with all predictors \nlinear_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# LASSO regression model\nlasso_model &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Random forest model\nrandom_forest_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", seed = rngseed) %&gt;%\n  set_mode(\"regression\")\n\n\nCreating workflows for the models.\n\n# Creating workflows\n# Define recipes\nrecipe &lt;- recipe(Y ~ ., data = data_wBMI)\nrecipe_lasso &lt;- recipe(Y ~ ., data = data_wBMI) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# Linear model workflow\nlinear_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(linear_model)\n\n# LASSO model workflow\nlasso_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_lasso) %&gt;%\n  add_model(lasso_model)\n\n# Random forest model workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(random_forest_model)\n\n\nFitting the models.\n\n# Fitting the models\n# Fit the linear model\nlinear_fit &lt;- fit(linear_workflow, data = data_wBMI)\n\n# Fit the LASSO model\nlasso_fit &lt;- fit(lasso_workflow, data = data_wBMI)\n\n# Fit the random forest model\nrf_fit &lt;- fit(rf_workflow, data = data_wBMI)\n\n\nMaking predictions for the models.\n\nlinear_preds &lt;- predict(linear_fit, new_data = data_wBMI) %&gt;%\n  bind_cols(data_wBMI) %&gt;%\n  rename(predicted = .pred)\n\n# Predictions for the LASSO model\nlasso_preds &lt;- predict(lasso_fit, new_data = data_wBMI) %&gt;%\n  bind_cols(data_wBMI) %&gt;%\n  rename(predicted = .pred)\n\n# Predictions for the random forest model\nrf_preds &lt;- predict(rf_fit, new_data = data_wBMI) %&gt;%\n  bind_cols(data_wBMI) %&gt;%\n  rename(predicted = .pred)\n\n\nCalculating the RMSE for each model.\n\n# Calculating RMSE for each model\nlinear_rmse &lt;- rmse(linear_preds, truth = Y, estimate = predicted) # Linear Model\nlasso_rmse &lt;- rmse(lasso_preds, truth = Y, estimate = predicted) # LASSO Model\nrf_rmse &lt;- rmse(rf_preds, truth = Y, estimate = predicted) # Random Forest Model\n\n\nVisualizing model fit by plotting predicted vs observed values for each model.\n\n# Plotting first fit\n# Define colors and shapes for the legend\ncolors &lt;- c(\"Predicted\" = \"#5c88da\", \"Observed\" = \"#ffcd00\", \"Reference Line\" = \"gray\")\n\n# Linear Model\np_linear &lt;- ggplot(data_wBMI, aes(x = Y)) +\n  geom_point(aes(y = linear_preds$predicted, color = \"Predicted\"), show.legend = FALSE) +\n  geom_point(aes(y = Y, color = \"Observed\"), show.legend = FALSE) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = colors[\"Reference Line\"]) +\n  labs(title = \"Linear Model\", y = \"Predicted\", x = \"Observed\") +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  theme(aspect.ratio = 1) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme(axis.title.x = element_text(size = 8), \n        axis.title.y = element_text(size = 8)) +\n  theme(text = element_text(family = 'mono')) +\n  theme(title = element_text(size = 10))\n\n\n# LASSO Model\np_lasso &lt;- ggplot(data_wBMI, aes(x = Y)) +\n  geom_point(aes(y = lasso_preds$predicted, color = \"Predicted\")) +\n  geom_point(aes(y = Y, color = \"Observed\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = colors[\"Reference Line\"]) +\n  labs(title = \"LASSO Model\", y = \"Predicted\", x = \"Observed\", color = \"Legend\") +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  theme(aspect.ratio = 1) +\n  theme(legend.text = element_text(size = 6)) +\n  theme(legend.title = element_text(size = 6)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme(axis.title.x = element_text(size = 8), \n        axis.title.y = element_text(size = 8)) +\n  theme(text = element_text(family = 'mono')) +\n  theme(title = element_text(size = 10))\n\n\n# Random Forest Model\np_rf &lt;- ggplot(data_wBMI, aes(x = Y)) +\n  geom_point(aes(y = rf_preds$predicted, color = \"Predicted\"), show.legend = FALSE) +\n  geom_point(aes(y = Y, color = \"Observed\"), show.legend = FALSE) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = colors[\"Reference Line\"]) +\n  labs(title = \"RF Model\", y = \"Predicted\", x = \"Observed\") +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  theme(aspect.ratio = 1)+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme(axis.title.x = element_text(size = 8), \n        axis.title.y = element_text(size = 8)) +\n  theme(text = element_text(family = 'mono')) +\n  theme(title = element_text(size = 10))\n\n# Combine plots\np_combined &lt;- (p_rf+ p_linear + p_lasso) + plot_layout(ncol = 3)\np_combined &lt;- p_combined + \n  plot_annotation(caption = 'First Fit - Predicted vs Observed Values') +\n  theme(plot.caption = element_text(size = 20), text = element_text(family = 'mono'))\n\nprint(p_combined)\n\n\n\n\n\nPrinting a table of RMSE values for each model.\n\n# Create a data frame with your RMSE values\nrmse_data &lt;- data.frame(\n  Model = c(\"Linear Regression\", \"Lasso Regression\", \"Random Forest\"),\n  RMSE = c(linear_rmse$.estimate, lasso_rmse$.estimate, rf_rmse$.estimate)\n)\n\n# Print the table using kable and kableExtra\nkable(rmse_data, \"html\", align = \"c\", caption = \"RMSE Values for Different Models\") %&gt;%\n  kable_styling(full_width = TRUE)\n\n\nRMSE Values for Different Models\n\n\nModel\nRMSE\n\n\n\n\nLinear Regression\n571.5954\n\n\nLasso Regression\n571.6504\n\n\nRandom Forest\n358.8240\n\n\n\n\n\n\n\n\n\nThe LASSO model and linear model perform similarly in this case. This could be due to the fact that in cases where there is little multicollinearity among predictors and the underlying model is not too complex, the LASSO model tends to perform in a similar way to the linear regression model.\n\n\n\n\n\n\n\n\n\nDefining penalty grid values and adjusting the model for tuning.\n\n# Define a grid of penalty values correctly\npenalty_grid &lt;- grid_regular(penalty(range = c(log10(1E-5), log10(1E2))), levels = 50)\n\n# Adjust the LASSO model specification for tuning\nlasso_model_tune &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n\nCreating workflow and resampling using the apparent function.\n\n# Create workflow\nlasso_workflow_tune &lt;- workflow() %&gt;%\n  add_recipe(recipe_lasso) %&gt;%  # Make sure recipe_lasso is defined correctly\n  add_model(lasso_model_tune)\n\n# Create an apparent resamples object for the apparent error rate\napparent_resamples &lt;- apparent(data_wBMI)\n\n\nTuning the model and visualizing with autoplot.\n\n# Tune the model\ntuning_results &lt;- tune_grid(\n  object = lasso_workflow_tune,\n  resamples = apparent_resamples,\n  grid = penalty_grid\n)\n\n\n# Visualize the tuning process\nlassobadplot &lt;- autoplot(tuning_results)\nlassobadplot + ggtitle(\"LASSO Model Tuned (Bad)\")\n\n\n\n\n\n\n\n\nLASSO does best (lowest RMSE) for low penalty values and gets worse as the penalty value increases. RMSE only increases and does not go lower than the value from the linear model. Why?\nAs the penalty parameter in LASSO increases, the model simplifies by reducing the number of features. It does this to prevent overfitting, but in some cases it can lead to underfitting. The lowest RMSE that LASSO can achieve will not be lower than the linear regression model because the penalty introduces bias which reduces variance.\n\n\n\n\n\n\nDefining model with tunable parameters and creating a tuning grid.\n\n# Define the random forest model with tunable parameters\nrandom_forest_model_tune &lt;- rand_forest(trees = 300, mtry = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\", seed = rngseed) %&gt;%\n  set_mode(\"regression\")\n\n# Create a tuning grid\ntuning_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7\n)\n\n\nCreating a workflow and tuning the random forest model, then visualizing with autoplot.\n\n# Create workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(random_forest_model_tune)\n\n# Tune random forest model\ntune_res &lt;- tune_grid(\n  rf_workflow,\n  resamples = vfold_cv(data_wBMI, v = 5), # Adjust the number of folds if necessary\n  grid = tuning_grid\n)\n\n\n# Visualizing with autoplot\nrfbadplot &lt;- autoplot(tune_res)\nrfbadplot + ggtitle(\"Random Forest Model Tuned (Bad)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefining penalty grid values and adjusting the model for tuning.\n\n# Define a grid of penalty values correctly\npenalty_grid &lt;- grid_regular(penalty(range = c(log10(1E-5), log10(1E2))), levels = 50)\n\n# Adjust the LASSO model specification for tuning\nlasso_model_tune_cv &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n\nAdjusting the workflow and creating 5-fold cross-validation resamples.\n\n# Adjust the workflow for the tunable model\nlasso_workflow_tune_cv &lt;- workflow() %&gt;%\n  add_recipe(recipe_lasso) %&gt;%  # Make sure recipe_lasso is defined correctly\n  add_model(lasso_model_tune_cv)\n\n# Create the cross-validation resamples\ncv_resamples &lt;- vfold_cv(data_wBMI, v = 5, repeats = 5)\n\n\nTuning the LASSO model using cross-validation and then visualizing with autoplot.\n\n# Tune LASSO model using cross-validation \ntuning_results_lasso_cv &lt;- tune_grid(\n  object = lasso_workflow_tune_cv,\n  resamples = cv_resamples,\n  grid = penalty_grid\n)\n\n# Plot the tuning results\nlassocvplot &lt;- autoplot(tuning_results_lasso_cv)\nlassocvplot + ggtitle(\"LASSO Model Tuned w/ CV\")\n\n\n\n\n\n\n\n\n\nDefining model with tunable parameters and creating a tuning grid.\n\n# Define the random forest model with tunable parameters\nrandom_forest_model_tune_cv &lt;- rand_forest(trees = 300, mtry = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\", seed = rngseed) %&gt;%\n  set_mode(\"regression\")\n\n# Create a tuning grid\ntuning_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7\n)\n\n\nCreating a workflow and tuning the random forest model using cross-validation, then visualizing with autoplot.\n\n# Create workflow\nrf_workflow_cv &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(random_forest_model_tune_cv)\n\n# Tune the random forest model\ntune_res_cv &lt;- tune_grid(\n  rf_workflow_cv,\n  resamples = cv_resamples,\n  grid = tuning_grid\n)\n\n# Plot the tuning results\nrfcvplot &lt;- autoplot(tune_res_cv)\nrfcvplot + ggtitle(\"Random Forest Model Tuned w/ CV\")\n\n\n\n\n\n\n\n\n\n\nThe LASSO models both follow a similar trend, the lower values are better and both perform worse with too much regularization. However, the “Bad” version seemed to initally be a better fit until the end where it began to overfit and then severely underperform.\n\n\n\nThe RF models also both follow a similar trend, as the number of randomly selected predictors increases, the RMSE decreases and stabalizes and the R-square value increases with the number of predictors. However, the “Bad” version has overall higher RMSE values and lower R-square values, which could indicate more overfitting.\n\n\n\n\n\n\nLASSO performs the best. Based on the autoplot graph, the LASSO model has a lower RMSE compared to the RF model’s best RMSE values. Similarly the LASSO model has smaller R-squared values than the RF model."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Summary/Abstract",
    "section": "",
    "text": "PATRICK KAGGWA contributed to this exercise."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Summary/Abstract",
    "section": "General Background Information",
    "text": "General Background Information\nThis exercise was done as part of an assignment and is educational in nature. The premise of the assignment was the cleaning, processing, and analysis of a provided “unclean” data set via manipulation with R and RStudio. The exercise was also meant to encourage collaboration between classmates, in preparation for future collaboration involving scientific data."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Summary/Abstract",
    "section": "Description of data and data source",
    "text": "Description of data and data source\nData included variables of ‘height’, ‘weight’, ‘gender’, ‘age’ and ‘hair color’. Each variable had 14 associated values which were either numeric or categorical.\nThe data was provided via Andreas Handel and Liza Hall."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-acquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-acquisition",
    "title": "Summary/Abstract",
    "section": "Data acquisition",
    "text": "Data acquisition\nData for ‘Height’, ‘Weight’, and ‘Gender’ were provided by Andreas Handel.\nData for ‘Age’ and ‘Hair Color’ were provided by Liza Hall, and were chosen using random generators available online (age generator)."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Summary/Abstract",
    "section": "Data import and cleaning",
    "text": "Data import and cleaning\nThis part of the exercise was done by Patrick Kaggwa. All files can be found on Github in the ‘starter-analysis-exercise’ folder.\nData was imported, processed, and cleaned in RStudio using the following script:\n---\ntitle: \"An example cleaning script\"\nauthor: \"Patrick Kaggwa\"\ndate: \"2023-01-03\"\noutput: html_document\n---\n\n\n# Processing script\n\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\n# Setup\n\nLoad needed packages. make sure they are installed.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n```\n:::\n\n\n# Data loading\n\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so:\npackage::function() that's not required one could just call the function specifying the package makes it clearer where the function \"lives\",\nbut it adds typing. You can do it either way.\n\n::: {.cell}\n\n```{.r .cell-code}\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n```\n:::\n\n\n# Check data\n\nFirst we can look at the codebook\n\n::: {.cell}\n\n```{.r .cell-code}\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                         `Allowed Values`\n  &lt;chr&gt;           &lt;chr&gt;                                         &lt;chr&gt;           \n1 Height          height in centimeters                         numeric value &gt;…\n2 Weight          weight in kilograms                           numeric value &gt;…\n3 Gender          identified gender (male/female/other)         M/F/O/NA        \n4 Age             age in years                                  numeric value &gt;0\n5 Hair Color      hair color (black, brown, blonde, red, other) Blk, Bro, Bln, …\n```\n:::\n:::\n\n\nSeveral ways of looking at the data\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::glimpse(rawdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 14\nColumns: 5\n$ Height       &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"…\n$ Weight       &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender       &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"…\n$ Age          &lt;dbl&gt; 26, 41, 58, 18, 58, 59, 24, 30, 20, 57, 40, 41, 57, 43\n$ `Hair Color` &lt;chr&gt; \"Bro\", \"Bro\", \"Bro\", \"Oth\", \"Bro\", \"Bln\", \"Blk\", \"Bro\", \"…\n```\n:::\n\n```{.r .cell-code}\nsummary(rawdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :18.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:27.00  \n Mode  :character   Median :  70.0   Mode  :character   Median :41.00  \n                    Mean   : 602.7                      Mean   :40.86  \n                    3rd Qu.:  90.0                      3rd Qu.:57.00  \n                    Max.   :7000.0                      Max.   :59.00  \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n```\n:::\n\n```{.r .cell-code}\nhead(rawdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  Height Weight Gender   Age `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M         26 Bro         \n2 175        70 O         41 Bro         \n3 sixty      60 F         58 Bro         \n4 178        76 F         18 Oth         \n5 192        90 NA        58 Bro         \n6 6          55 F         59 Bln         \n```\n:::\n\n```{.r .cell-code}\nskimr::skim(rawdata)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |        |\n|:------------------------|:-------|\n|Name                     |rawdata |\n|Number of rows           |14      |\n|Number of columns        |5       |\n|_______________________  |        |\n|Column type frequency:   |        |\n|character                |3       |\n|numeric                  |2       |\n|________________________ |        |\n|Group variables          |None    |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Height        |         0|             1|   1|   5|     0|       13|          0|\n|Gender        |         0|             1|   1|   2|     0|        5|          0|\n|Hair Color    |         0|             1|   3|   3|     0|        5|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|      sd| p0| p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|------:|-------:|--:|---:|---:|---:|----:|:-----|\n|Weight        |         1|          0.93| 602.69| 1922.25| 45|  55|  70|  90| 7000|▇▁▁▁▁ |\n|Age           |         0|          1.00|  40.86|   15.25| 18|  27|  41|  57|   59|▆▂▅▂▇ |\n:::\n:::\n\n\n\n# Cleaning\n\nBy inspecting the data as done above, we find some problems that need addressing:\n\nFirst, there is an entry for height which says \"sixty\" instead of a number. \nDoes that mean it should be a numeric 60? It somehow doesn't make sense since the weight is 60kg, which can't happen for a 60cm person (a baby).\nSince we don't know how to fix this, we might decide to remove the person. This \"sixty\" entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn't very meaningful. So let's fix that first.\n\n::: {.cell}\n\n```{.r .cell-code}\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |d1   |\n|Number of rows           |13   |\n|Number of columns        |5    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |2    |\n|numeric                  |3    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Gender        |         0|             1|   1|   2|     0|        5|          0|\n|Hair Color    |         0|             1|   3|   3|     0|        5|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|      sd| p0|    p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|------:|-------:|--:|------:|---:|---:|----:|:-----|\n|Height        |         0|          1.00| 151.62|   46.46|  6| 154.00| 165| 175|  192|▁▁▁▂▇ |\n|Weight        |         1|          0.92| 647.92| 2000.48| 45|  54.75|  73|  90| 7000|▇▁▁▁▁ |\n|Age           |         0|          1.00|  39.54|   15.02| 18|  26.00|  41|  57|   59|▇▂▆▂▇ |\n:::\n\n```{.r .cell-code}\nhist(d1$Height)\n```\n\n::: {.cell-output-display}\n![](starter-analysis-report_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don't know, we might need to remove this person, which we'll do here.\n\n::: {.cell}\n\n```{.r .cell-code}\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |d2   |\n|Number of rows           |13   |\n|Number of columns        |5    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |2    |\n|numeric                  |3    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Gender        |         0|             1|   1|   2|     0|        5|          0|\n|Hair Color    |         0|             1|   3|   3|     0|        5|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|      sd|  p0|    p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|------:|-------:|---:|------:|---:|---:|----:|:-----|\n|Height        |         0|          1.00| 165.23|   16.52| 133| 155.00| 166| 178|  192|▂▇▆▆▃ |\n|Weight        |         1|          0.92| 647.92| 2000.48|  45|  54.75|  73|  90| 7000|▇▁▁▁▁ |\n|Age           |         0|          1.00|  39.54|   15.02|  18|  26.00|  41|  57|   59|▇▂▆▂▇ |\n:::\n:::\n\nHeight values seem ok now.\n\nNow let's look at the `Weight` variable. There is a person with weight of 7000, which is impossible, and one person with missing weight.\nTo be able to analyze the data, we'll remove those individuals as well.\n\n::: {.cell}\n\n```{.r .cell-code}\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |d3   |\n|Number of rows           |11   |\n|Number of columns        |5    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |2    |\n|numeric                  |3    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Gender        |         0|             1|   1|   2|     0|        5|          0|\n|Hair Color    |         0|             1|   3|   3|     0|        4|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|    sd|  p0|   p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|------:|-----:|---:|-----:|---:|---:|----:|:-----|\n|Height        |         0|             1| 167.09| 16.81| 133| 155.5| 166| 179|  192|▂▇▅▇▅ |\n|Weight        |         0|             1|  70.45| 20.65|  45|  54.5|  70|  85|  110|▇▂▃▃▂ |\n|Age           |         0|             1|  37.91| 15.40|  18|  25.0|  41|  50|   59|▇▂▃▂▆ |\n:::\n:::\n\nNow checking the `Gender` variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\n::: {.cell}\n\n```{.r .cell-code}\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |d3   |\n|Number of rows           |11   |\n|Number of columns        |5    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |1    |\n|factor                   |1    |\n|numeric                  |3    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Hair Color    |         0|             1|   3|   3|     0|        4|          0|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts             |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------------|\n|Gender        |         0|             1|FALSE   |        5|M: 4, F: 3, O: 2, N: 1 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|    sd|  p0|   p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|------:|-----:|---:|-----:|---:|---:|----:|:-----|\n|Height        |         0|             1| 167.09| 16.81| 133| 155.5| 166| 179|  192|▂▇▅▇▅ |\n|Weight        |         0|             1|  70.45| 20.65|  45|  54.5|  70|  85|  110|▇▂▃▃▂ |\n|Age           |         0|             1|  37.91| 15.40|  18|  25.0|  41|  50|   59|▇▂▃▂▆ |\n:::\n:::\n\n\nNow we see that there is another NA, but it's not `NA` from R, instead it was loaded as character and is now considered as a category.\nWell proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I'm also using droplevels() to get rid of it.\n\n::: {.cell}\n\n```{.r .cell-code}\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |d4   |\n|Number of rows           |9    |\n|Number of columns        |5    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |1    |\n|factor                   |1    |\n|numeric                  |3    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Hair Color    |         0|             1|   3|   3|     0|        4|          0|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts       |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------|\n|Gender        |         0|             1|FALSE   |        3|M: 4, F: 3, O: 2 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|    sd|  p0| p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|------:|-----:|---:|---:|---:|---:|----:|:-----|\n|Height        |         0|             1| 165.67| 15.98| 133| 156| 166| 178|  183|▂▁▃▃▇ |\n|Weight        |         0|             1|  70.11| 21.25|  45|  55|  70|  80|  110|▇▂▃▂▂ |\n|Age           |         0|             1|  37.67| 14.35|  18|  26|  41|  43|   59|▇▂▅▂▅ |\n:::\n:::\n\n\nAll done, data is clean now. \n\nLet's assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\n::: {.cell}\n\n```{.r .cell-code}\nprocesseddata &lt;- d4\n```\n:::\n\n\n# Save data \n\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. \nThis preserves coding like factors, characters, numeric, etc.  If you save as CSV, that information would get lost.\nHowever, CSV is better for sharing with others since it's plain text. If you do CSV, you might want to write down somewhere what each variable is.\n\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n```\n:::\n\nNote the use of the `here` package and `here` command to specify a path relative to the main project directory, that is the folder that contains the `.Rproj` file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\n\n# Notes\n\nRemoving anyone observation with \"faulty\" or missing data is one approach. It's often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Summary/Abstract",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nStatistical analysis for Table 1 was done using the following code.\n############################\n#### Third model fit\n# fit linear model using height as outcome, hair color and age as predictors\n\nlmfit3 &lt;- lm(Height ~ `Hair Color` + `Age`, mydata)  \n\n# place results from fit into a data frame with the tidy function\nlmtable3 &lt;- broom::tidy(lmfit3)\n\n#look at fit results\nprint(lmtable3)\n\n# save fit results table  \ntable_file3 = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"resulttable3.rds\")\nsaveRDS(lmtable3, file = table_file3)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#figures-and-tables",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#figures-and-tables",
    "title": "Summary/Abstract",
    "section": "Figures and Tables",
    "text": "Figures and Tables\nFigures were generated by Patrick Kaggwa, tables were produced by Liza Hall.\n\nFigure 1. Height distribution histogram\n\nFigure 2. Weight distribution histogram\n\nFigure 3. Scatter plot with linear regression fit depicting the relationship between height and weight\n\nFigure 4. Scatter plot with gender-stratified linear regression fits illustrating the relationship between height and weight\nA tibble 5x5\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np. value\n\n\n1\n(Intercept)\n156.\n25.5\n6.12\n0.00367\n\n\n2\n‘Hair Color’ Bln\n2.06\n30.0\n0.0687\n0.949\n\n\n3\n‘Hair Color’ Bro\n15.8\n24.7\n0.639\n0.558\n\n\n4\n‘Hair Color’ Oth\n10.0\n25.5\n0.393\n0.714\n\n\n5\nAge\n-0.00238\n0.633\n-0.00376\n0.997\n\n\n\nTable 1. Table of outcomes from a fit linear model using height as outcome and hair color and age as predictors"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/lizahall/Desktop/School/Spring 2024/Applied Data Analysis/GitHub/lizahall-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   3   3     0        4          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  37.7 14.4  18  26  41  43   59 ▇▂▅▂▅\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/lizahall/Desktop/School/Spring 2024/Applied Data Analysis/GitHub/lizahall-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/lizahall/Desktop/School/Spring 2024/Applied Data Analysis/GitHub/lizahall-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                         `Allowed Values`\n  &lt;chr&gt;           &lt;chr&gt;                                         &lt;chr&gt;           \n1 Height          height in centimeters                         numeric value &gt;…\n2 Weight          weight in kilograms                           numeric value &gt;…\n3 Gender          identified gender (male/female/other)         M/F/O/NA        \n4 Age             age in years                                  numeric value &gt;0\n5 Hair Color      hair color (black, brown, blonde, red, other) Blk, Bro, Bln, …\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height       &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"…\n$ Weight       &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender       &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"…\n$ Age          &lt;dbl&gt; 26, 41, 58, 18, 58, 59, 24, 30, 20, 57, 40, 41, 57, 43\n$ `Hair Color` &lt;chr&gt; \"Bro\", \"Bro\", \"Bro\", \"Oth\", \"Bro\", \"Bln\", \"Blk\", \"Bro\", \"…\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :18.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:27.00  \n Mode  :character   Median :  70.0   Mode  :character   Median :41.00  \n                    Mean   : 602.7                      Mean   :40.86  \n                    3rd Qu.:  90.0                      3rd Qu.:57.00  \n                    Max.   :7000.0                      Max.   :59.00  \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M         26 Bro         \n2 175        70 O         41 Bro         \n3 sixty      60 F         58 Bro         \n4 178        76 F         18 Oth         \n5 192        90 NA        58 Bro         \n6 6          55 F         59 Bln         \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n40.86\n15.25\n18\n27\n41\n57\n59\n▆▂▅▂▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n39.54\n15.02\n18\n26.00\n41\n57\n59\n▇▂▆▂▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n39.54\n15.02\n18\n26.00\n41\n57\n59\n▇▂▆▂▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n3\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n37.91\n15.40\n18\n25.0\n41\n50\n59\n▇▂▃▂▆\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n3\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n37.91\n15.40\n18\n25.0\n41\n50\n59\n▇▂▃▂▆\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n3\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n37.67\n14.35\n18\n26\n41\n43\n59\n▇▂▅▂▅\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Liza's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  }
]